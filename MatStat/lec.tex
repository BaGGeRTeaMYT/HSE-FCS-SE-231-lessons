\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathabx}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{caption}
\usepackage[margin=0.25in]{geometry}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usepackage{nicefrac}
\graphicspath{../Images}
\usepackage{tikz}
\usepackage{cancel}

% \usepackage{fontspec}
% \setmainfont{Calibri}

\newcommand{\sion}{\sum\limits_{i = 1}^{n}}
\newcommand{\dev}{\mathcal{D}}
\newcommand{\cov}{\operatorname{cov}}
\newcommand{\real}{\mathbb{R}}

\DeclareMathOperator{\sign}{sign}

\title{Математическая статистика.}
\author{Андрей Тищенко \href{https://t.me/AndrewTGk}{@AndrewTGk}}
\date{2024/2025}

\begin{document}
\maketitle
\begin{center}
    Лекция 10 января
\end{center}
\section*{Преамбула}
\textit{Статистика}. Мнения о появлении этого слова:
\begin{enumerate}
    \item Статистиками в Германии назывались люди, собирающие данные о населении и передающие их государству.
    \item В определённый день в Венеции народ выстраивался для выплаты налогов (строго фиксированных, в зависимости от рода действий). Государство собирало данные обо всём населении. Это происходило до появления статистиков в Германии, поэтому мы будем считать, что статистика пошла из Венеции.
\end{enumerate}
\textit{Задача статистики} --- по результатам наблюдений построить вероятностную модель наблюдаемой случайной величины.
\section*{Основные определения}
\subsection*{Определение}
\underline{Однородной выборкой объёма $n$} называется случайный вектор $X = (X_1,\dots,\ X_n)$, компоненты которого являются независимыми и одинаково распределёнными. Элементы вектора $X$ называются \underline{элементами}\\\underline{выборки}. 
\subsection*{Определение}
Если элементы выборки имеют распределение $F_{\xi}(x)$, то говорят, что выборка \underline{соответствует распределению} $F_{\xi}(x)$ или \underline{порождена случайной величиной} $\xi$ с распределением $F_{\xi}(x)$. 
\subsection*{Определение}
Детерминированный вектор $x = (x_1,\dots,\ x_n)$, компоненты которого $x_i$ являются реализациями соответствующих случайных величин $X_i\ (i = \overline{1,\ n})$, называется \underline{реализацией выборки}.
\subsection*{Уточнение}
Если $X$ --- однородная выборка объёма $n$, то его реализацией будет вектор $x$, каждый элемент $x_i$ которого является значением соответствующей ему случайной величины (элемента выборки) $X_i$. 
\subsection*{Определение}
\underline{Выборочным пространством} называется множество всех возможных реализаций выборки\\
\[X = (X_1,\dots,\ X_n)\]
\subsection*{Пример}
У вектора $X = (X_1,\dots,\ X_{10})$ каждый элемент $X_i$ которой порождён случайной величиной $\xi\sim U(0,\ 1)$, выборочным пространством является $\mathbb{R}^{10}$ (так как $X_i$ может принять любое значение на $\mathbb{R}$)
\subsection*{Определение}
Обозначим $x_{(i)}$ --- $i$-ый по возрастанию элемент, тогда будет справедливо:
\[x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}\]
Обозначим $X_{(k)}$ случайную величину, реализация которой при каждой реализации $x$ выборки $X$ принимает значение $x_{(k)}$. Тогда последовательность $X_{(1)},\dots,\ X_{(n)}$ называется \underline{вариационным рядом выборки}.
\subsection*{Определение}
Случайная величина $X_{(k)}$ называется \underline{$k$-ой порядковой статистикой выборки}.
\subsection*{Определение}
Случайные величины $X_{(1)},\ X_{(n)}$ называются \underline{эстремальными порядковыми статистиками}.
\subsection*{Определение}
Порядковая статистика $X_{(\lceil n\cdot p \rceil)}$ называется \underline{выборочной квантилью уровня $p$}, где $p\in [0,\ 1]$
\subsection*{Определение}
Пусть каждый элемент выборки $X$ объёма n имеет распределение $F_{\xi}(x)$. \underline{Эмпирической функцией}\\ \underline{распределения} такой выборки называется
\[\hat{F}_n(x) = \frac{1}{n} \sum_{k = 1}^{n} I(X_k\leq x)\]
$I$ --- индикаторная функция. $I = \begin{cases}
    1,\ \text{если аргумент верен}\\
    0,\ \text{иначе}
\end{cases}$\\
Пусть $x_1,\dots,\ x_n$ --- реализация выборки $X_1,\dots,\ X_n$\\
\[
\begin{tikzpicture}[scale=2, >=latex]
    \draw[->, >=latex] (0, -1) -- (0, 1) node(0)[anchor = east]{$\hat{F}_n(x)$};
    \draw (-2, -0.05) node(1)[anchor=north]{$x_1$} -- (-2, 0.05);
    \draw (-1, -0.05) node(2)[anchor=north]{$x_2$}-- (-1, 0.05);
    \draw (0.07, -0.05) node(3)[anchor=north east]{$x_3$}-- (0.07, -0.05);
    \draw (1, -0.05) node(4)[anchor=north]{$x_4$}-- (1, 0.05);
    \draw (2, -0.05) node(5)[anchor=north]{$x_5$}-- (2, 0.05);
    \draw[->] (-3, 0) -- (-2, 0);
    \draw[->] (-2, 0.2) -- (-1, 0.2);
    \draw[dashed] (-2, 0) -- (-2, 0.2);
    \draw[->] (-1, 0.45) -- (0, 0.45);
    \draw[dashed] (-1, 0) -- (-1, 0.45);
    \draw[->] (0, 0.6) -- (1, 0.6);
    \draw[dashed] (1, 0) -- (1, 0.85);
    \draw[->] (1, 0.85) -- (2, 0.85);
    \draw[dashed] (2, 0) -- (2, 1);
    \draw[->] (2, 1) -- (3, 1);
    \draw[->] (-3, 0) -- (3, 0) node(2)[anchor = south]{$x$};
\end{tikzpicture}
\]
\begin{center}
    Свойства $\hat{F}_n(x)$
\end{center}
\begin{enumerate}
    \item $\displaystyle\forall x\in\mathbb{R}\quad E\hat{F}_n(x) = E\left(\frac{1}{n} \sum_{k = 1}^{n} I(X_k \leq x)\right) = \frac{1}{n} \sum_{k = 1}^{n} EI(X_k \leq x) = P(X_1 \leq x) = F_{\xi}(x)$
    \item По усиленному закону больших чисел (УЗБЧ)
    \[\forall x\in \mathbb{R}\quad \hat{F}_n(x) = \frac{1}{n} \sum_{k = 1}^{n} I(X_k \leq x) \xrightarrow[n\to\infty]{\text{п. н.}} EI(X_k \leq x) = F_{\xi}(x)\]
\end{enumerate}
\subsection*{Гистограмма}
Разбить $\mathbb{R}$ на $(m + 2)$ непересекающихся интервала. Рассматриваются $x_{(1)},\dots,\ x_{(m)}$
\[
\begin{tabular}{|c|c|c|}
    \hline
    Название & Обозначение & Формула\\
    \hline
    $\begin{aligned}&\text{Количество}\\&\text{интервалов}\end{aligned}$ & $m$ & ---\\
    \hline
    $\begin{aligned}&\text{Размах}\\&\text{выборки}\end{aligned}$ & $r$ & $r = x_{(m)} - x_{(1)}$\\
    \hline
    $\begin{aligned}&\text{Ширина}\\&\text{интервала}\end{aligned}$ & $\Delta$ & $\Delta = \frac{r}{m}$\\
    \hline
    $\begin{aligned}&\text{Количество}\\&\text{попаданий на}\\&\text{$i$-ый интервал}\end{aligned}$ & $\nu_i$ & ---\\
    \hline
    $\begin{aligned}&\text{Частота}\\&\text{попаданий на}\\&\text{$i$-ый интервал}\end{aligned}$ & $h_i$ & $h_i = \frac{\nu_i}{\Delta}$\\
    \hline
\end{tabular}\quad
\begin{tikzpicture}[scale=2, >=latex]
    \draw[->] (-2, 0) -- (2, 0) node(2)[anchor = south]{$x$};
    \draw (-1.5, -0.1) node[anchor=north](4){$x_{(1)}$} -- (-1.5, 0.1) node(3){};
    \draw (1.5, -0.1) node[anchor=north](6){$x_{(m)}$} -- (1.5, 0.1) node(5){};
    \draw[<->] (-1.5, -0.08) -- node[anchor=north](10){$r$} (1.5, -0.08);
    \draw (0.75, -0.05) -- (0.75, 0.05) node(6){};
    \draw[<->] (-0.75, 0.08) -- node[anchor=south](11){$\Delta$} (0, 0.08);
    \draw (0, -0.05) -- (0, 0.05) node(8){};
    \draw (-0.75, -0.05) -- (-0.75, 0.05) node(9){};
    \draw (-1.5, 0) rectangle (-0.75, 0.5);
    \draw[->] (0.375, 0) -- node(0)[anchor = west]{$h_i$} (0.375, 0.75);
    \draw (-0.75, 0) rectangle (-0.0, 0.8);
    \draw (-0.0, 0) rectangle (0.75, 0.75);
    \draw (0.75, 0) rectangle (1.5, 0.2);
\end{tikzpicture}
\]
\begin{center}
    Лекция 17 января
\end{center}
\subsection*{Определение}
Пусть $X_1,\dots,\ X_n \sim F(x,\ \theta)$. \underline{$k$-ым начальным выборочным моментом} называется \[\hat{\mu}_k = \frac{1}{n} \sum_{i = 1}^{n} x_i^k,\ k \in \mathbb{N}\]
\underline{Выборочным средним} называется:
\[\hat{\mu}_1 = \overline{X} = \frac{1}{n} \sum_{i = 1}^{n} x_i\]
\subsection*{Определение}
\underline{$k$-ым центральным выборочным моментом} называется 
\[\hat{\nu}_k = \frac{1}{n} \sum_{i = 1}^{n} {(x_i - \overline{X})}^k,\ k = 2,\ 3,\dots\]
\[\hat{\nu}_2 = S^2 = \frac{1}{n} \sum_{i = 1}^{n} {(x_i - \overline{X})}^2 \text{ называется выборочной дисперсией}\]
Пусть $(x_1,\ y_1),\dots,\ (x_n,\ y_n)$ соответствует распределению $F(x,\ y,\ \theta)$
\subsection*{Определение}
\underline{Выборочной ковариацией} называется
\[\hat{K}_{xy} = \frac{1}{n}\sum_{i = 1}^{n} (x_i - \overline{X}) (y_i - \overline{Y})\]
\subsection*{Определение}
\underline{Выборочным коэффициентом корреляции} называется 
\[\hat{\rho}_{xy} = \frac{\hat{K}_{xy}}{\sqrt{S_x^2 S_y^2}}\]
\begin{center}
    Свойства выборочных моментов
\end{center}
\begin{enumerate}
    \item $E\hat{\mu}_k = E\left( \frac{1}{n} \sum_{i = 1}^{n} X_i^k \right) = \frac{1}{n} \sum_{i = 1}^{n} E X_i^k = EX_1^k = \mu_k$
    \item $E\overline{X} = m_x$
    \item $\mathcal{D} \hat{\mu}_k = \mathcal{D} \left( \frac{1}{n} \sum_{i = 1}^{n} x_i^k \right) = \frac{1}{n^2} \sum_{i = 1}^{n} \mathcal{D} X_i^k = \frac{1}{n} \mathcal{D} X_i^k = \frac{1}{n} \left( EX_1^{2k} - {(EX_1^{K})}^2 \right) = \frac{1}{n}(\mu_{2k} - \mu_k^2)$
    \item $\mathcal{D}\overline{X} = \frac{\sigma^2_{x_1}}{n}$
    \item По УЗБЧ
    \[\hat{\mu}_k = \frac{1}{n} \sum_{i = 1}^{n} x_i^k \xrightarrow[n\to \infty]{\text{п. н.}} E\hat{\mu}_k = \mu_k\]
    \[\hat{\nu}_k \xrightarrow[n\to \infty]{\text{п. н.}} \nu_k\]
    \item По ЦПТ
    \[\frac{\hat{\mu}_k - \mu_k}{\sqrt{\frac{\mu_{2k} - \mu_k^2}{n} }}\xrightarrow[d]{n\to\infty}U,\ U\sim N(0,\ 1)\]
    \[\frac{\sqrt{n}(\overline{X} - m_{x_1})}{\sigma} \xrightarrow[n\to\infty]{d} U\]
    \item $ES^2 = E\left( \frac{1}{n} \sum_{i = 1}^{n}{(x_i - \overline{X})}^2 \right) =  \frac{1}{n} E\left( \sion \left(x_i^2 - 2x_i \overline{X} + \overline{X}^2\right) \right) = E(x^2) - \frac{2}{n} \sion E(x_i\overline{X}) + \frac{1}{n}\sion E\overline{X}^2 =\\
    = E(x^2) - \frac{2}{n} \sion E x_i \sum\limits _{j = 1}^{n} x_j + \frac{1}{n} \sion E {\left( \sum\limits_{j = 1}^{n} x_j \right)}^2 = E(x^2) - \frac{2}{n} E\sum\limits_{i = 1}^{n} x_i \sum_{j = 1}^{n} x_j +  = \frac{n - 1}{n} \sigma^2$
    \item $E \hat{K}_{xy} = \frac{n - 1}{n} \cov(x,\ y)$
\end{enumerate}
\subsection*{Определение}
Оценкой $\hat{\theta}$ параметра $\theta$, называется функция:
\[\hat{\theta} = T(x_1,\dots,\ x_n),\ \text{не зависящая от $\theta$}\]
Например, отвратительная оценка среднего роста людей в аудитории.
\[\hat{m} = \frac{2x_2 + 5x_5 + 10x_{10}}{3}\]
\subsection*{Определение}
Оценка $\hat{\theta}$ называется \underline{несмещённой}, если $E\hat{\theta} = \theta$ для любых возможных значений этого параметра.
\subsection*{Определение}
Оценка $\hat{\theta}(x_1,\dots,\ x_n)$ называется \underline{асимптотически несмещённой оценкой} $\theta$, если
\[\lim_{n\to\infty} E\hat{\theta}(x_1,\dots,\ x_n) = \theta\]
\[\lim_{n\to\infty} ES^2 = \lim_{n\to\infty} \frac{n - 1}{n}\sigma^2 = \sigma^2\]
\subsection*{Определение}
\underline{Несмещённой выборочной} (или исправленной) \underline{выборочной дисперсией} называется
\[\tilde{S}^2 = \frac{1}{n - 1} \sum_{i = 1}^{n} {(x_i - \overline{x})}^2\]
Оценки
\[\begin{aligned}
    \hat{m}_1 &= \frac{x_1 + x_2 + x_3}{3}\\
    \hat{m}_2 &= \frac{\sum_{i = 1}^{10} x_i}{10}\\
    \hat{m}_3 &= \overline{x} = \frac{\sion x_i}{n}\\  
\end{aligned}\]
Являются несмещёнными.
\subsection*{Определение}
Оценка $\hat{\theta}(x_1,\dots,\ x_n)$ называется:\\
\underline{Состоятельной оценкой} $\theta$, если
\[\hat{\theta} (x_1,\dots,\ x_n) \xrightarrow[n\to\infty]{p} \theta\]
\underline{Сильно состоятельной оценкой}, если
\[\hat{\theta} (x_1,\dots,\ x_n) \xrightarrow[n\to\infty]{\text{п. н.}} \theta\]
\subsection*{Определение}
Пусть $\hat{\theta}$ --- несмещённая оценка параметра $\theta$. Если $\dev\hat{\theta} \leq \dev \theta^*$, где $\theta^*$ --- любая несмещённая оценка параметра $\theta$. Тогда $\hat{\theta}$ называется \underline{эффективной оценкой параметра} $\theta$.
\begin{center}
    $R$-эффективные оценки
\end{center}
Рассматриваем выборку $X_1,\ldots,\ X_n \sim f(x,\ \theta),\ \theta\in \Theta \subseteq \mathbb{R}^1$. Назовём модель $(S,\ f(x,\ \theta))$ \underline{регулярной}, если она удовлетворяет следующим условиям:
\begin{enumerate}
    \item $\forall x\in S\quad \text{функция } f(x,\ \theta) = f(x_1,\dots,\ x_n,\ \theta) > 0$ и дифференцируема по $\theta$.
    \item $\begin{cases} \displaystyle\frac{\delta}{\delta \theta} \int\limits_S f(x,\ \theta)\, dx = \int\limits_S \frac{\delta}{\delta \theta} f(x,\ \theta)\, dx\\
        \displaystyle \frac{\delta}{\delta \theta} \int\limits_S T(x)f(x,\ \theta)\, dx = \int\limits_S \frac{\delta}{\delta \theta} T(x) f(x,\ \theta)\, dx \end{cases}$
\end{enumerate}
Пусть $\hat{\theta} = T(x) = T(x_1,\dots,\ x_n)$ --- несмещённая оценка параметра $\theta$:
\[\int\limits_S \frac{\delta}{\delta \theta} f(x,\ \theta)\, dx = \frac{\delta}{\delta \theta} \int\limits_S f(x,\ \theta)\, dx = \frac{\delta}{\delta \theta} 1 = 0\]
\[\int\limits_S \frac{\delta}{\delta \theta} T(x) f(x,\ \theta)\, dx =  \frac{\delta}{\delta \theta} \int\limits_S T(x) f(x,\ \theta) \, dx = \frac{\delta}{\delta\theta} ET(x) = \frac{\delta}{\delta \theta}\theta = 1\]
\subsection*{Определение}
\underline{Информацией Фишера} о параметре $\theta$, содержащейся в выборке $X_1,\dots,\ X_n$ называется величина
\[I_n(\theta) = E{\left( \frac{\delta \ln \big(f(x,\ \theta)\big)}{\delta \theta} \right)}^2 = \int\limits_S{\left( \frac{\delta \ln \big(f(x,\ \theta)\big)}{\delta \theta} \right)}^2 f(x,\ \theta)\, dx\]
\subsection*{Неравенство Рао-Крамера}
Если $S,\ f(x,\ \theta)$ --- регулярная модель и $\hat{\theta}$ --- несмещённая оценка $\theta$, то 
\[\dev (\hat{\theta}) \geq \frac{1}{I_n(\theta)}\]
\textbf{Доказательство}\\
Выпишем некоторые равенства (пригодятся в доказательстве):
\[\int\limits_S \frac{\delta}{\delta \theta} f(x,\ \theta)\, dx = \int\limits_S \frac{\delta f(x,\ \theta)}{\delta \theta} \frac{f(x,\ \theta)}{f(x,\ \theta)}\, dx \overset{*}{=} \int\limits_S \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx = 0\]
Пояснение $\overset{*}{=}$. Логарифм --- сложная функция. По правилу дифференцирования сложной функции:
\[\frac{\delta \ln f(x,\ \theta)}{\delta \theta} = \frac{1}{f(x,\ \theta)} \cdot \frac{\delta f(x,\ \theta)}{\delta \theta}\]
\[\int\limits_S \frac{\delta}{\delta \theta} T(x) f(x,\ \theta)\, dx = \int\limits_S T(x) \frac{\delta}{\delta \theta} f(x,\ \theta) \frac{f(x,\ \theta)}{f(x,\ \theta)}\, dx = \int\limits_S T(x) \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx = 1\]
Чуть преобразуем последнее полученное равенство:
\[\int\limits_S T(x) \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx = \int\limits_S T(x) \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx - \theta\underset{=0}{\underbrace{\int\limits_S \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx}} = \]
\[= \int\limits_S \big( T(x) - \theta \big) \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx = 1 \Rightarrow 1 = 1^2 = {\left( \int\limits_S \big( T(x) - \theta \big) \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx \right)}^2 \]
Далее нам понадобится неравенство Коши-Буняковского, которое выглядит так:
\[{\left( \int \varphi_1(x)\varphi_2(x)\, dx \right)}^2 \leq \int \varphi_1^2(x)\, dx\int \varphi_2^2(x)\, dx\]
Подгоним полученное равенство $\left(f(x,\ \theta) > 0 \Rightarrow f(x,\ \theta) = \sqrt{f(x,\ \theta)}^2\right)$:
\[{\left( \int\limits_S \big( T(x) - \theta \big) \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta)\, dx \right)}^2 = {\left( \int\limits_S \underset{\varphi_1(x)}{\underbrace{\big( T(x) - \theta \big) \sqrt{f(x,\ \theta)}}}\cdot\underset{\varphi_2(x)}{\underbrace{\frac{\delta \ln f(x,\ \theta)}{\delta \theta} \sqrt{f(x,\ \theta)}}}\, dx \right)}^2 = 1\]
И применим неравенство Коши-Буняковского:
\[1 = {\left( \int\limits_S \underset{\varphi_1(x)}{\underbrace{\big( T(x) - \theta \big) \sqrt{f(x,\ \theta)}}}\cdot\underset{\varphi_2(x)}{\underbrace{\frac{\delta \ln f(x,\ \theta)}{\delta \theta} \sqrt{f(x,\ \theta)}}}\, dx \right)}^2 \leq\]
\[\leq \int\limits_S {\left( (T(x) - \theta)\sqrt{f(x,\ \theta)} \right)}^2 \, dx \cdot \int\limits_S {\left( \frac{\delta \ln f(x,\ \theta)}{\delta \theta} \sqrt{f(x,\ \theta)} \right)}^2 \, dx =\]
\[= \underset{=\dev\hat{\theta}}{\underbrace{\int\limits_S {\big( T(x) - \theta \big)}^2 f(x,\ \theta)\, dx}} \cdot \underset{=I_n(\theta)}{\underbrace{\int\limits_S{\left( \frac{\delta \ln \big(f(x,\ \theta)\big)}{\delta \theta} \right)}^2 f(x,\ \theta)\, dx}}\]
Получаем:
\[1 \leq \dev (\theta) \cdot I_n(\theta)\Rightarrow \dev (\theta) \geq \frac{1}{I_n(\theta)}\]
\subsection*{Определение}
Оценка $\hat{\theta}$ называется \underline{$R$-эффективной}, если $E\hat{\theta} = \theta$ и $\dev \hat{\theta} = \frac{1}{I_n(\theta)}$
\begin{center}
    Лекция 24 января
\end{center}
\subsection*{Замечание 1}
$I_n(\theta) = \dev \left( \frac{\delta \ln f(x,\ \theta)}{\delta \theta} \right)$
\subsection*{Замечание 2}
$I_n(\theta) = nI_1(\theta)$\\
$\displaystyle f(x,\ \theta) = f(x_1,\dots,\ x_n,\ \theta) = \prod_{i = 1}^{n} f(x_i,\ \theta)\\
E{\left( \frac{\delta \ln f(x,\ \theta)}{\delta \theta} \right)}^2 = E \left( \sion \frac{\delta \ln f(x_i,\ \theta)}{\delta \theta} \right)^2 = \sum_{i \neq j} E \left( \frac{\delta \ln f(x_i,\ \theta)}{\delta \theta} \cdot \frac{\delta \ln f(x_j,\ \theta)}{\delta \theta} \right) + nE\left( \frac{\delta \ln f(x_1, \theta)}{\delta \theta} \right)^2 =\\
= \sum_{i \neq j} \left( \underset{=0}{\underbrace{E \left( \frac{\delta \ln f(x_i,\ \theta)}{\delta \theta}\right)}} \cdot \underset{=0}{\underbrace{E\left( \frac{\delta \ln f(x_j,\ \theta)}{\delta \theta} \right)}}\right) + nE\left( \frac{\delta \ln f(x_1, \theta)}{\delta \theta} \right)^2 = nE\left( \frac{\delta \ln f(x_1, \theta)}{\delta \theta} \right)^2 = nI_1(\theta)$
\subsection*{Замечание 3}
Пример: $X_1,\dots,\ X_n \sim N(\theta,\ \sigma^2)$\\
Рассмотрим оценку $\hat{\theta} = \overline{X}$, её дисперсия $\mathcal{D}\overline{X} = \frac{\sigma^2}{n}$. Посчитаем информацию Фишера:\\
$I_1(\theta) = E\left( \frac{\delta \ln f(x,\ \theta)}{\delta \theta} \right)^2 = E\left( \frac{\delta}{\delta \theta} \ln\left( \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x - \theta)^2}{2\sigma^2}}\right) \right)^2 = E\left( \frac{\delta}{\delta \theta} \ln\left( \frac{1}{\sqrt{2\pi} \sigma} - \frac{(x - \theta)^2}{2\sigma^2} \right) \right)^2 = E\left( \frac{x - \theta}{\sigma^2} \right)^2 = \\
=\frac{1}{\sigma^4} E(x - \theta)^2 = \frac{\sigma^2}{\sigma^4} = \frac{1}{\sigma^2}\Rightarrow I_n(\theta) = \frac{n}{\sigma^2}$\\
Знаем, что $\mathcal{D}\hat{\theta} \geq \frac{1}{nI_1(\theta)} = \frac{\sigma^2}{n} = \mathcal{D}(\overline{X})\Rightarrow$ оценка $\hat{\theta} = \overline{X}$ является R-эффективной.\\
Критерий эффективности $X_1,\dots,\ X_n \sim F_{\xi}(x,\ \theta),\ \theta \in \Theta \subset \mathbb{R}^1$ выполнены условия регулярности, то есть
\[ \int T(x)\frac{\delta f(x,\ \theta)}{\delta \theta}\, dx = \frac{\delta}{\delta \theta} \int T(x) f(x,\ \theta)\, dx = E\hat{\theta}\]

\subsection*{Определение}
Функцией вклада выборки $X_1,\dots,\ X_n$ называется
\[U(x,\ \theta) = \sion \frac{\delta \ln f(x_i,\ \theta)}{\delta \theta}\]
Пусть $0 < U(x, \theta) < \infty$.\\
$\hat{\theta} = T(x_1,\dots,\ x_n)$ --- R-эффективная оценка $\theta \Leftrightarrow \hat{\theta} - \theta = a(\theta) U(x,\ \theta)$, где $a(\theta) = \mathcal{D}\hat{\theta}$\\
\textit{Доказательство} $\Rightarrow$:\\
Пусть $\hat{\theta} - \theta = a(\theta) U(x,\ \theta) \Rightarrow \hat{\theta}$ --- R-эффективная оценка $\theta$.\\
Посчитаем математическое ожидание частей равенства:
\[E(\hat{\theta} - \theta) = a(\theta) EU(x,\ \theta) = a(\theta) \int \frac{\delta \ln f(x,\ \theta)}{\delta \theta} f(x,\ \theta) dx = 0\]
Посчитаем дисперсию частей:
\[\dev (\hat{\theta} - \theta) = a^2(\theta) \dev U(x,\ \theta) = \underset{=(\dev(\hat{\theta}))^2}{\underbrace{a^2(\theta)}} I_n(\theta)\Rightarrow \dev(\hat{\theta}) = (\dev(\hat{\theta}))^2 I_n(\theta)\Rightarrow 1 = \dev(\theta) I_n(\theta)\]
Значит оценка является R-эффективной.\\
\textit{Доказательство} $\Leftarrow$:\\
Пусть $\hat{\theta}$ --- R-эффективная оценка $\Rightarrow \hat{\theta} - \theta = a(\theta) U(x,\ \theta)$. Хотим доказать, что $\rho(\hat{\theta},\ U(x,\ \theta)) = 1$.\\
Для подсчёта корреляции нужно посчитать ковариацию:
\[\cov (\hat{\theta},\ U(x,\ \theta)) = E(\hat{\theta} - \theta) U(x,\ \theta) = E\hat{\theta} U(x,\ \theta) - \theta \underset{=0}{\underbrace{EU(x,\ \theta)}} =\]
\[= \int\limits_S T(x)U(x,\ \theta) f(x,\ \theta)\, dx = \int\limits_S T(x) \frac{\delta \ln f(x,\ \theta)}{\delta \theta}f(x,\ \theta)\, dx = 1\]
Так как $\hat{\theta}$ --- R-эффективная оценка, то $\dev \hat{\theta} = \frac{1}{I_n(\theta)}$. Знаем, что $\dev U(x,\ \theta) = I_n(\theta)$, тогда:
\[\rho(\hat{\theta},\ U(x,\ \theta)) = \frac{\cov (\hat{\theta},\ U(,\ \theta))}{\sqrt{\dev \hat{\theta} \dev U(x,\ \theta)}} = \frac{1}{\sqrt{\frac{I_n(\theta)}{I_n(\theta)}}} = 1\Rightarrow\]
\[\Rightarrow \hat{\theta} = c_1 + c_2U(x,\ \theta)\]
$E\hat{\theta} = c_1 + Ec_2 U(x,\ \theta) = c_1 + 0 = \theta$, так как оценка эффективная\\
$\dev \hat{\theta} = c_2^2 I_n(\theta) = \frac{1}{I_n(\theta)}\Rightarrow c_2^2 = \frac{1}{I_n^2}\Rightarrow c_2 = \frac{1}{I_n} = \dev\hat{\theta} = a(\theta)$.\\
Итак, $\hat{\theta} = \theta + a(\theta) U(x,\ \theta)\Rightarrow \hat{\theta} - \theta = U(x,\ \theta)$.
\subsection*{Метод моментов}
$X_1,\dots,\ X_n\sim F_{\xi}(x,\ \theta),\ \theta \in \Theta\subset R^k$
\[\exists \mu_j < \infty,\ j = \overline{1,\ k}\quad \underset{=\mu_j(\theta)}{\underbrace{\mu_j}} = E\xi^j = \int\limits_{-\infty}^{+\infty} x^j f(x,\ \theta)\, dx = 1\]
Тогда можно получить систему уравнений:
\begin{equation}
    \begin{cases}
        \hat{\mu}_1 = \mu_1(\theta)\\
        \vdots\\
        \hat{\mu}_k = \mu_k(\theta)
    \end{cases}
\end{equation}
Если система уравнений $(1)$ однозначно разрешима относительно $\theta_1,\dots,\ \theta_k$, то решения $\hat{\theta_1},\dots,\ \hat{\theta_k}$ называется равной $\theta_1,\dots,\ \theta_k$ по методу моментов.
\subsubsection*{Пример}
$X_1,\dots,\ X_n \sim N(\theta_1,\ \theta_2^2),\ \theta = (\theta_1,\ \theta_2^2)$, тогда:
\[\begin{cases}
    \hat{\mu}_1 = \frac{1}{n} \sion x_i = \theta_1\Rightarrow \hat{\theta_1} = \overline{X}\\
    \hat{\mu}_2 = \frac{1}{n} \sion x_i^2 = \theta_2^2 + \theta_1^2,\ \big(E\xi^2 = \dev \xi + (E\xi)^2\big)\Rightarrow \hat{\theta}_2^2 = \frac{1}{n}\sion  x_i^2 = \overline{X}^2
\end{cases}\]
\subsection*{Метод максимального правдоподобия (ММП)}
\subsubsection*{Определение}
Функцией правдоподобия для $X_1,\dots,\ X_n$, порождённых случайной величиной $\xi$, называется функция 
\[L(x_1,\dots,\ x_n,\ \theta) = \begin{cases}
    \prod\limits_{i = 1}^{n} f(x_i,\ \theta),\ \text{если $\xi$ --- непрерывная случайная величина}\\
    \prod\limits_{i = 1}^{n} P(\xi = x_i,\ \theta),\ \text{если $\xi$ --- дискретная случайная величина}
\end{cases}\]
\subsubsection*{Определение}
Реализацией оценки максимального правдоподобия (ОМП) называется значение $\hat{\theta} \in \Theta$, такое что:
\[\hat{\theta} = \operatorname{argmax} L(x_1,\dots,\ x_n,\ \theta),\ \text{где }\theta \in \Theta\]
Для нахождения точки максимума нужно взять частные производные по всем составляющим $\theta$ от функции правдоподобия. Однако считать производную произведения нам впадлу, поэтому мы введём следующую вещь:
\subsubsection*{Определение}
Функция $\ln L(x_1,\dots,\ x_n,\ \theta)$ называется логарифмической функцией правдоподобия.\\
Итак, получаем систему уравнений:
\[\begin{cases}
    \frac{\delta \ln L(x_1,\dots,\ x_n,\ \theta)}{\delta \theta_1} = 0\\
    \vdots\\
    \frac{\delta \ln L(x_1,\dots,\ x_n,\ \theta)}{\delta \theta_k} = 0
\end{cases}\]
Логарифм монотонный, поэтому его argmax совпадёт с argmax функции $L(x_1,\dots,\ x_n,\ \theta)$ (НАУКА!).
\subsubsection*{Пример}
Для Гауссовской величины $N(\theta_1,\ \theta_2^2)$:
\[L(x_1,\dots,\ x_n,\ \theta) = \prod\limits_{i = 1}^{n} \frac{1}{\sqrt{2\pi}\sigma^2} e^{-\frac{(x - \theta_1)^2}{2\theta_2^2}} = \left( \frac{1}{\sqrt{2\pi}} \right)^n \left( \frac{1}{\theta_2} \right)^n e^{-\frac{(x - \theta_1)^2}{2\theta_2^2}}\]
Логарифмируем:
\[\ln L(x_1,\dots,\ x_n,\ \theta) = \ln \left( \frac{1}{\sqrt{2\pi}} \right)^n - n\ln \theta_2 - \frac{\sion (x_i - \theta_1)^2}{2\theta_2^2}\]
Возьмём частные производные:
\[\begin{cases}
    \frac{\delta \ln L(x_1,\dots,\ x_n,\ \theta)}{ \delta \theta_1} = \frac{\sion (x_i - \hat{\theta}_1)}{\hat{\theta}_2^2}\\
    \frac{\delta \ln L(x_1,\dots,\ x_n,\ \theta)}{\delta \theta_2} = -\frac{n}{\hat{\theta}_2} + \frac{\sion (x_i - \hat{\theta}_1)^2}{\hat{\theta_2}^3}
\end{cases}\]
Посчитаем $\theta_1,\ \theta_2$:
\[\begin{cases}
    \sion (x_i - \hat{\theta}_1) = 0 \Rightarrow \hat{\theta}_1 = \frac{1}{n} \sion x_i = \overline{X}\\
    -n\hat{\theta}_2^2 + \sion (x_i - \overline{x})^2 = 0 \Rightarrow \hat{\theta}_2^2 = \frac{1}{n} \sion (x_i - \overline{x})^2
\end{cases}\]
\begin{center}
    \bf Лекция 31 января.
\end{center}
\subsection*{Робастные оценки}
От слова robust.\\
\subsection*{Определение}
Пусть оценка $\hat{\theta}_n$ построена по выборке $X_1,\dots,\ X_n$. Затем добавлено наблюдение $x$ и построена оценка $\hat{\theta}_{n + 1}$, тогда \underline{кривой чувствительности}, изучающей влияние наблюдения $x$ на оценку $\hat{\theta}$ называется функция:
\[S C_n (x) = \frac{\hat{\theta}_{n + 1} - \hat{\theta}_n}{\frac{1}{n + 1}} = (n + 1)\left( \hat{\theta}_{n + 1} - \hat{\theta}_n \right)\]
\subsection*{Определение}
Оценка $\hat{\theta}$ называется $B$-робастной, если $SC_n(x)$ ограничена.
\subsubsection*{Пример}
Пусть $\hat{\theta} = \overline{X}$
\[SC_n(x) = (n + 1)\left( \frac{1}{n + 1} \left( \sion (x_i) + x \right) - \frac{1}{n} \sion x_i \right) = \sion x_i + x - \left( \sion x_i + \frac{1}{n} \sion x_i \right) = x - \overline{X}\]
Это линейная функция от $x$, то есть кривая чувствительности неограничена.\\
Пусть $\hat{\theta} = \hat{\mu}$ (выборочная медиана)
\[\hat{\mu} = \begin{cases}
    X_{(k + 1)},\ n = 2k + 1\\
    \frac{X_{(k)} + X_{(k + 1)}}{2},\ n = 2k
\end{cases}\]
\subsubsection*{Определение}
Пороговой точкой (BP) $\varepsilon^*_n$ оценки $\hat{\theta}$, построенной на выборке $X_1,\dots,\ X_n$ называется:
\[\varepsilon^*_n = \frac{1}{n} \max\left\{ m:\ \max_{i_1,\dots,\ i_m} \sup_{y_1,\dots,\ y_m} |\hat{\theta} (z_1,\dots,\ z_m)| < \infty \right\}\]
Где выборка $z_1,\dots,\ z_m$ получена заменой значений $X_{i_1},\dots,\ X_{i_m}$ на произвольные значения $y_1,\dots,\ y_m$
\subsection*{Доверительные интервалы}
\subsection*{Определение}
Пусть для $X_1,\dots,\ X_n \sim F(x,\ \theta),\ \theta \subset \Theta \subset \mathbb{R}^1$ построены статистики $T_1(x_1,\dots,\ x_n)$ и $T_2(x_1,\dots,\ x_n)$, такие что
\[\begin{cases}
T_1(x) < T_2(x)\\
P\big( T_1(x) < \theta < T_2(x)\big) = 1 - \alpha,\ 0 < \alpha < 1
\end{cases}\]
Тогда интервал $\big( T_1(x),\ T_2(x) \big)$ называется доверительным интервалом уровня надёжности (доверия) $1 - \alpha$ параметра $\theta$.
\subsection*{Определение}
Случайная функция $G(x_1,\dots,\ x_n,\ \theta) = G(x,\ \theta)$ называется центральной (опорной) статистикой, если 
\begin{enumerate}
    \item $G(x,\ \theta)$ непрерывна и монотонна по $\theta$
    \item $F_G(x)$ не зависит от $\theta$
\end{enumerate}
Односторонние доверительные интервалы:
\[P\big( G(x,\ \theta) < Z_{1 - \alpha} \big) = 1 - \alpha\]
\[P\big( Z_{\alpha} < G(x,\ \theta) \big) = 1 - \alpha\]
Квантили не зависят от $\theta$, с их помощью можно выразить односторонние доверительные интервалы.\\
Центральным доверительным интервалом будет:
\[P\big( Z_{\frac{\alpha}{2}} < G(x,\ \theta) < Z_{1 - \frac{\alpha}{2}} \big) = 1 - \alpha\]
\subsection*{Определение}
Пусть случайные величины $\xi_1,\dots,\ \xi_m \sim N(0,\ 1)$ и независимы.\\
Тогда случайная величина $\eta = \sum\limits_{i = 1}^{m} \xi_i^2 \sim \chi^2 (m)$ (удовлетворяет распределению хи-квадрат ($\chi^2$) с $m$ степенями свободы).
\subsection*{Определение}
Пусть $\xi_0,\ \xi_1,\dots,\ \xi_m \sim N(0,\ 1)$ и независимы.\\
Тогда случайная величина $\zeta = \frac{\xi_0}{\sqrt{\frac{1}{m} \sum_{i = 1}^{m} \xi_i^2}} \sim t(m)$ (распределение Стьюдента с $m$ степенями свободы)
\subsection*{Определение}
Пусть случайная величина $\xi_1 \sim \chi^2(m),\ \xi_2 \sim \chi^2(n)$ и $\xi_1$ и $\xi_2$ --- независимы. Тогда случайная величина $F = \frac{\frac{1}{m} \xi_1}{\frac{1}{n} \xi_2} \sim F(m,\ n)$ (распредление Фишера со степенями свободы $n,\ m$)
\subsection*{Теорема Фишера}
Пусть $X_1,\dots,\ X_n$ порождены случайной величиной $X \sim N(m,\ \sigma^2)$, тогда:
\begin{enumerate}
    \item $\frac{nS^2}{\sigma^2} = \sion \left( \frac{x_i - \overline{x}}{\sigma} \right)^2 \sim \chi^2(n - 1)$ (так как мы знаем $\overline{X}$, и все наблюдения, а по $n - 1$ наблюдению и $\overline{X}$ можно восстановить последнее наблюдение)
    \item $\overline{X}$ и $S^2$ --- независимые случайные величины.
\end{enumerate}
\subsection*{Пример 1}
$X_1,\dots,\ X_n \sim N(\theta,\ \sigma^2),\ \sigma^2$ --- известно. Построить доверительный инртервал для $\theta$
\[\hat{\theta} = \overline{X} \sim N(\theta,\ \frac{\sigma^2}{n})\]
\[ \frac{\sqrt{n} (\overline{X} - \theta)}{\sigma} \sim N(0,\ 1) \]
\[P\left( Z_{\frac{\alpha}{2}} < \frac{\sqrt{n} (\overline{X} - \theta)}{\sigma} < Z_{1 - \frac{\alpha}{2}} \right) = 1 - \alpha\]
Поскольку по середине стоит стандартное гауссовское распределение: $Z_{\frac{\alpha}{2}} = -Z_{1 - \frac{\alpha}{2}}$ 
\[P\left( -\frac{Z_{1 - \frac{\alpha}{2}}\sigma}{\sqrt{n}} - \overline{X} < -\theta < \frac{Z_{1 - \frac{\alpha}{2}} \sigma}{\sqrt{n}} - \overline{X} \right) = 1 - \alpha\]
\[P\left( \overline{X} - \frac{Z_{1 - \frac{\alpha}{2}}\sigma}{\sqrt{n}}  < \theta < \overline{X} + \frac{Z_{1 - \frac{\alpha}{2}} \sigma}{\sqrt{n}} \right) = 1 - \alpha\]
Итак, доверительный интервал: $\left( \overline{X} - \frac{Z_{1 - \frac{\alpha}{2}}\sigma}{\sqrt{n}},\ \overline{X} + \frac{Z_{1 - \frac{\alpha}{2}}\sigma}{\sqrt{n}}\right)$
\subsection*{Пример 2}
$X_1,\dots,\ X_n \sim N(m,\ \theta_2^2)$. Построить доверительный интервал для $\theta_2^2$
\[\sum_{i = 1}^n \left( \frac{x_i - m}{\theta_2} \right)^2 \sim \chi^2(n)\]
\[P\left(\chi^2_{n,\ \frac{\alpha}{2}} < \frac{\sion (x_i - m)^2}{\theta_2^2} < \chi^2_{n,\ 1 - \frac{\alpha}{2}}\right) = 1 - \alpha \]
\[P\left( \frac{\sion (x_i - m)^2}{\chi^2_{n,\ 1 - \frac{\alpha}{2}}} < \theta_2^2 < \frac{\sion (x_i - m)^2}{\chi^2_{n,\ \frac{\alpha}{2}}} \right) = 1 - \alpha\]
Здесь $\chi^2_{n,\ \alpha}$ --- квантиль уровня $\alpha$ распределения $\chi^2(n)$
\subsection*{Пример 3}
Если нам неизвестны оба параметра $N(\theta_1,\ \theta_2^2)$. Заменяем $m$ на $\overline{X}$:
Доверительный интервал для $\theta_2$:
\[P\left( \frac{\sion (x_i - \overline{X})^2}{\chi^2_{n,\ 1 - \frac{\alpha}{2}}} < \theta_2^2 < \frac{\sion (x_i - \overline{X})^2}{\chi^2_{n,\ \frac{\alpha}{2}}} \right) = 1 - \alpha\]
Доверительный интервал для $\theta_1$:
\[\frac{\sqrt{n} \left(\frac{\overline{X} - \theta}{\sigma}\right)}{ \sqrt {\frac{1}{n - 1} \sum\left(\frac{(x_i - \overline{X})}{\sigma} \right)^2}} = \frac{\sqrt{n} (\overline{X} - \theta_1)}{\tilde{S}} \sim t(n - 1)\]
Обозначим $t_{n,\ \alpha}$ квантиль уровня $\alpha$ распределения $t(n)$, заметим, что $t_{n,\ 1 - \alpha} = t_{n,\ 1 - \frac{\alpha}{2}}$
\[P(t_{n,\ 1 - \frac{\alpha}{2}} < \frac{\sqrt{n}(\overline{X} - \theta_1)}{\tilde{S}} < t_{n,\ \frac{\alpha}{2}} ) = 1 - \alpha\]
\[P(\overline{X} - \frac{\tilde{S}\cdot t_{n,\ 1 - \frac{\alpha}{2}}}{\sqrt{n}} < \theta_1 < \overline{X} + \frac{\tilde{S}\cdot t_{n,\ 1 - \frac{\alpha}{2}}}{\sqrt{n}} ) = 1 - \alpha\]
\begin{center}
    Лекция 7 февраля
\end{center}
\subsection*{Задача}
$X_1,\dots,\ X_{n_1} \sim N(m_1,\ \sigma_1^2)$ и $Y_1,\dots,\ Y_{n_2} \sim N(m_2,\ \sigma_2^2)$. $\sigma$ известны, $m$ --- неизвестны.\\
$X_1,\dots,\ X_n$ и $Y_1,\dots,\ Y_n$ независимы. Доверительнный интервал для $\theta = m_1 - m_2$
\[T(x,\ y) = \frac{\overline{X} - \overline{Y} - \theta}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}\]
\subsection*{Задача}
Пусть $X_1,\dots,\ X_{n_1} \sim N(m_1,\ \sigma^2),\ Y_1,\dots,\ Y_{n_2} \sim N(m_2,\ \sigma^2)$. $\sigma$ неизвестна. Выборки независимы.
\subsubsection*{Утверждение}
\[\frac{\sum_{i = 1}^{n_1} {(x_i - \overline{X})}^2}{\sum_{i = 1}^{n_2}{(y_i - \overline{Y})}^2} \sim F(n_1 - 1,\ n_2 - 1)\]
\[\frac{\overline{X} - \overline{Y} - (m_1 - m_2)}{\sqrt{\hat\dev (\overline{X} - \overline{Y})}}\]
Посчитаем дисперсию в знаменателе:
\begin{equation*}
    \begin{aligned}
        \dev (\overline{X} - \overline{Y}) &= \dev \overline{X} + \dev \overline{Y} = \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2} = \sigma^2\left( \frac{1}{n_1} + \frac{1}{n_2} \right)\\
        S^2 &= \frac{\sum_{i = 1}^{n_1} {(x_i - \overline{X})}^2 + \sum_{i = 1}^{n_2} {(y_i - \overline{Y})}^2 }{n_1 + n_2 - 2}
    \end{aligned}    
\end{equation*}
Тогда
\[\frac{\overline{X} - \overline{Y} - (m_1 - m_2)}{\sqrt{\hat\dev (\overline{X} - \overline{Y})}} = \frac{\overline{X} - \overline{Y} - (m_1 - m_2)}{S\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)\]
Теперь можно построить доверительный интервал:
\[P\left( -t_{1 - \alpha/2,\ n_1 + n_2 - 2} < \frac{\overline{X} - \overline{Y} - \theta}{S\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} < t_{1 - \alpha/2,\ n_1 + n_2 - 2}\right) = 1 - \alpha\]
\[P\left(-t_{1 - \alpha/2,\ n_1 + n_2 - 2}\cdot S\sqrt{\frac{1}{n_1} + \frac{1}{n_2}} - (\overline{X} - \overline{Y}) < -\theta < t_{1 - \alpha/2,\ n_1 + n_2 - 2}\cdot S\sqrt{\frac{1}{n_1} + \frac{1}{n_2}} - (\overline{X} - \overline{Y})\right) = 1 - \alpha\]
\[P\left((\overline{X} - \overline{Y}) -t_{1 - \alpha/2,\ n_1 + n_2 - 2}\cdot S\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}  < \theta < t_{1 - \alpha/2,\ n_1 + n_2 - 2}\cdot S\sqrt{\frac{1}{n_1} + \frac{1}{n_2}} + (\overline{X} - \overline{Y})\right) = 1 - \alpha\]
\subsection*{Асимптотические доверительные интервалы}
Пусть $X_1,\dots,\ X_n\sim F(x,\ \theta),\ \theta \in \Theta \subset \mathbb{R}^1$\\
$\hat \theta$ --- состоятельная оценка $\theta$.
\[\sqrt{n}(\hat\theta_n - \theta) \xrightarrow[n\to\infty]{d} U,\ U \sim N\big(0,\ \sigma^2 (\theta)\big)\]
И $\sigma^2(\theta)$ непрерывна по $\theta$.
\[P\left( Z_{\alpha/2} < \frac{\sqrt{n}(\hat \theta_n - \theta)}{\sigma(\hat\theta_n)} < Z_{1 - \alpha/2} \right) \to 1 - \alpha\]
\[P\left( \hat\theta_n - \frac{\sigma(\hat\theta_n) Z_{1 - \alpha/2}}{\sqrt{n}} < \theta < \frac{\sigma(\hat\theta_n) Z_{1 - \alpha/2}}{\sqrt{n}} + \hat\theta_n\right)\]
Если $\exists$ R-эффективная оценка $\hat \theta_n$, то выбирая её $\dev \hat \theta_n = \frac{1}{I_n(\theta)}$, тогда $\frac{\sigma(\hat\theta_n)}{\sqrt{n}} = \sqrt{\dev \hat\theta_n} = \frac{1}{\sqrt{nI_1(\hat\theta_n)}}$
\[P\left( \hat\theta_n - \frac{Z_{1 - \alpha/2}}{\sqrt{n I_1 (\hat \theta_n)}} < \theta < \hat\theta_n + \frac{ Z_{1 - \alpha/2}}{\sqrt{n I_1(\hat \theta_n)}} \right) \to 1 - \alpha\]
\subsection*{Пример}
$X_1,\dots,\ X_n \sim Bi(1,\ \theta)$\\
АДИ для $\theta$:
\[\hat\theta = \frac{\sum_{i = 1}^{n} x_i}{n} - \text{несмещённая, состоятельная, R-эффективная}\]
$\dev x_i = \theta(1 - \theta)$.
\[\sqrt{n} (\hat \theta - \theta) \xrightarrow[n\to \infty]{d} U,\ U\sim N\big(0,\ \theta(1 - \theta)\big)\]
\[P\left( \hat\theta - Z_{1 - \alpha/2} \sqrt{\frac{\hat\theta(1 - \hat\theta)}{n}} < \theta < \hat \theta + Z_{1 - \alpha/2} \frac{\sqrt{\hat{\theta}(1 - \hat \theta)}}{\sqrt{n}} \right) \to 1 - \alpha\]
\subsection*{Определение}
Основная (или нулевая) гипотеза $H_0$, с ней конкурируют $H_1,\ H_2,\dots,\ H_{A}$ (альтернативные гипотезы).
\subsection*{Определение}
Сложной гипотезой называют гипотезу, которая не определяет параметры распределения или само распределение однозначно.\\
Например
\[H_1: \xi \sim N(m,\ \sigma^2)\]
\[H_2: \xi \sim N(5,\ \sigma^2)\]
Простая гипотеза определяет распределение однозначно, например:
\[H_3: \xi \sim N(5,\ 36)\]
Односторонние гипотезы выглядят так:
\[H_4: \xi m < 5\]
\[H_5: \xi m > 5\]
Двусторонние:
\[H_6: n \neq 5\]
\[H_7: m\in [1,\ 3]\]
А гипотеза $H_8: \{\text{``Сегодня хорошая погода''}\}$ не является статистической, ведь не относится к распределению и параметрам.
\subsection*{Определение}
Статистическим критерием называют правило, руководствуясь которым, на основании реализации $x_1,\dots,\ x_n$ выборки $X_1,\dots,\ X_n$ принимается решение о справедливости/несправедливости гипотезы $H_0$.\\
Делим множество реализаций выборки $S$ на два множества $S_0,\ S_1$, такие что
\[S_0 \cdot S_1 = \emptyset\]
\[S_0 + S_1 = S\]
Назовём $S_0$ доверительной областью, а $S_1$ --- критической областью. Если реализация попала в $S_0$, то мы принимаем $H_0$, иначе принимает альтернативную гипотезу.\\
Тогда ошибкой первого рода (уровнем значимости критерия) называется
\[P(X\in S_1\ |\  \text{верна } H_0) = \alpha\]
Ошибкой второго рода называется 
\[P(X\in S_0 \wedge \text{верна } H_1) = 1 - \beta\]
\subsection*{Определение}
Пусть критерий предназначен для проверки $H_0: \theta = \theta_0$ против альтернативы $H_1: \theta \neq \theta_0$, тогда функцией мощности критерия называется
\[\beta(\theta) = P(X \in S_1,\ \theta)\]
Критерий называется состоятельным, если при отдалении от $\theta_0$ его функция мощности стремится к $1$.
\begin{center}
    \bf Лекция 13 февраля
\end{center}
\subsection*{Проверка статистических гипотез}
Если $\beta$ --- функция мощности критерия проверки гипотезы $H_0: \theta = \theta_0$, тогда $\beta(\theta) = P(X\in S_1,\ \theta)$ и $\beta(\theta_0) = \alpha$, где $\alpha$ --- вероятность ошибки первого рода.
\subsection*{Задача}
$H_0: \theta = \theta_0$ и $H_1: \theta \in \Theta_1$, $\theta_0 \notin \Theta_1$. Пусть зафиксировано $\alpha > 0$, тогда критерий называется несмещённым, если:
\begin{equation*}
    \begin{aligned}
        \beta(\theta) \leq&\, \alpha,\ \text{если }\theta =  \theta_0\\
        \beta(\theta) >&\, \alpha,\ \text{если } \theta \in \Theta_1
    \end{aligned}
\end{equation*}
\subsection*{Определение}
Критерий, предназначенный для проверки $H_0: \theta = \theta_0$ против $H_1: \theta \in \Theta_1$ называется состоятельным, если
\[\forall \theta \in \Theta_1 \quad \beta (\theta) \xrightarrow[n\to\infty]{} 1,\ \text{где $n$ --- количество испытаний}\]
\subsection*{Определение}
Критерий $\beta_0$ называется равномерно наиболее мощным, если среди всех критериев $\beta$:
\[\forall \theta \in \Theta \quad \beta_0(\theta) \geq \beta(\theta)\]
Локально наиболее мощным, если
\[\forall \theta \in \Theta_1 \subseteq \Theta \quad \beta_0(\theta) \geq \beta(\theta)\]
\subsection*{Алгоритм проверки параметрических гипотез}
\begin{enumerate}
    \item Сформулировать проверяемую гипотезу $H_0$ и альтернативную к ней $H_1$.
    \item Выбрать уровень значимости $\alpha$
    \item Выбрать статистику $T$ для проверки гипотезы $H_0$
    \item Найти распределение $F(z\ |\ H_0)$ статистики $T$, при условии $\{\text{``$H_0$ верна''}\}$
    \item Построить, в зависимости от формулировки гипотезы $H_1$ и уровня значимости $\alpha$, критическую область $\overline{G}$
    \item Получить реализацию выборки наблюдений $x_1,\dots,\ x_n$ и вычислить реализацию $t = \varphi(x_1,\dots,\ x_n)$ статистики $T$ критерия
    \item Принять статистическое решение на уровне доверия $1 - \alpha$: если $t\in \overline{G}$, то отклонить гипотезу $H_0$ как не согласующуюся с результатами наблюдений, а если $t\in G$, то принять гипотезу $H_0$ как не противоречащую результатам наблюдений.
\end{enumerate}
\subsection*{Задача}
Дамы оценивают чай. Могут ли из двух чашек выбрать чашку с хорошим чаем?\\
Проводятся наблюдения $X_1,\dots,\ X_n \sim Bi(1,\ p)$
\begin{enumerate}
    \item $H_0:\ p = p_0 = 0.5,\ H_1: p > 0.5$. То есть $H_0$ --- дамы не могут выбрать (просто пытаются угадать).
    \item $\alpha = 0.05$. Так как специально указано не было, берём стандартное значение.
    \item $T(x) = \sion x_i$
    \item $T(x\ |\ H_0) \sim Bi(n,\ \frac{1}{2})$. Если $n$ велико:
    \[\frac{T(x) - np_0}{\sqrt{np_0 (1 - p_0)}} = \tilde T(x) \sim N(0,\ 1)\]
    \item Доверительная область: $[0,\ Z_{0,95}] = [0,\ 1.65]$. Критическая область: $(1.65,\ +\infty)$
    \item Пусть у нас есть данные $n = 30,\ \sum_{i = 1}^{30} x_i = 20 = T(x)$
    \[\tilde T(x) = \frac{20 - 30\cdot\frac{1}{2}}{\sqrt{30\cdot0.5\cdot 0.5}} \approx 1.82574\]
    \item Попали в критическую область, значит принимаем $H_1$ на уровне доверия $1 - \alpha = 0.95$
\end{enumerate}
\subsection*{Задача}
А если у нас есть две серии различных испытаний Бернулли?\\
Пусть $\xi_1 \sim Bi(n_1,\ p_1)$ и $\xi_2 \sim Bi(n_2,\ p_2)$. Хотим проверить $H_0: p_1 = p_2$ против альтернатив $H_1: p_1 < p_2,\ H_2: p_1 > p_2,\ H_3: p_1 \neq p_2$.\\
Введём обозначение $\hat p_1 = \frac{\sum_{i = 1}^{n_1} x_{i\, 1} }{n_1},\ \hat p_2 = \frac{\sum_{i = 1}^{n_2} x_{i\, 2}}{n_2}$, тогда:
\[\frac{\hat p_1 - \hat p_2}{\sqrt{\dev (\hat p_1 - \hat p_2)}} \sim N(0,\ 1)\]
Посчитаем $\dev (\hat p_1 - \hat p_2) = \dev (\hat p_1) + \dev(\hat p_2) - 2\underset{=0}{\underbrace{\cov (\hat p_1,\ \hat p_2)}} =\frac{p_1 q_1}{n_1} + \frac{p_2 q_2}{n_2} = pq \left(\frac{1}{n_1} + \frac{1}{n_2}\right)$. \\
Оценим $p$:
\[\hat p = \frac{\sum\limits_{i = 1}^{n_1} x_{i\, 1} + \sum\limits_{i = 1}^{n_2} x_{i\, 2}}{n_1 + n_2}\]
Тогда $\tilde T(x) = \frac{\hat p_1 - \hat p_2}{\sqrt{\hat p (1 - \hat p)\left( \frac{1}{n_1} + \frac{1}{n_2} \right)}}$. По этой статистике уже можем принимать решения.

\begin{center}
\bf Лекция 21 февраля
\end{center}
\subsection*{Лемма Неймана-Пирсона}
Пусть $X_1,\dots,\ X_n \sim f(x,\ \theta)$, параметр $\theta$ неизвестен. Проверяется простая гипотеза $H_0: \theta = \theta_0$ против простой альтернативной гипотезы $H_1: \theta = \theta_1$ $(\text{БОО}\ \theta_1 > \theta_0)$.\\
Существует наиболее мощный критерий для проверки $H_0$ против $H_1$ с критической областью\\
$\displaystyle S^*_{1\, \alpha} = \{(x_1,\dots,\ x_n)\ |\ T(x_1,\dots,\ x_n) \geq c_{\alpha}\}$, где $T(x_1,\dots,\ x_n) = \frac{L(x_1,\dots,\ x_n,\ \theta_1)}{L(x_1,\dots,\ x_n,\ \theta_0)} = \frac{\prod_{i = 1}^{n} f(x_i,\ \theta_1)}{\prod_{i = 1}^{n} f(x_i,\ \theta_0)}$, а $c_{\alpha}$ такое что $P_{\theta_0}(T(x) \geq c_{\alpha}) = \alpha$  
\subsubsection*{Доказательство}
Пусть есть критерий с критической областью $S_{1\, \alpha}$ лучше (более мощный) предложенного нашей леммой.\\
Тогда (под $x$ далее понимается вектор $(x_1,\dots,\ x_n)$):
\[\beta (\theta_1,\ S_{1\, \alpha}) = \int\limits_{S_{1\, \alpha}} \prod_{i = 1}^{n} f(x_i,\ \theta_1)\, dx_1\dots dx_n = \int\limits_{S_{1\, \alpha}} L(x,\ \theta_1)\, dx = \int\limits_{S_{1\,\alpha}S^*_{1\,\alpha}} L(x,\ \theta_1)\, dx + \int\limits_{S_{1\, \alpha} \overline{S}^*_{1\, \alpha}} L(x,\ \theta_1)\, dx \color{red}=\]
По определению функции $T(x)$:
\[T(x) = \frac{L(x,\ \theta_1)}{L(x,\ \theta_0)}\Rightarrow T(x)L(x,\ \theta_0) = L(x,\ \theta_1)\]
Подставим это в сумму:
\[{\color{red} =} \int\limits_{S_{1\,\alpha}S^*_{1\,\alpha}} T(x) L(x,\ \theta_0)\, dx + \int\limits_{S_{1\, \alpha} \overline{S}^*_{1\, \alpha}} T(x) L(x,\ \theta_0)\, dx\]
По определению $\beta(\theta, S_1) = P\left( X \in S_1,\ \theta \right)$ то есть правдоподобие попадания случайной величины в критическую область при заданном параметре.
\[\beta (\theta_1,\ S^*_{1\, \alpha}) = \int\limits_{S^*_{1\, \alpha}} L(x,\ \theta_1)\, dx = \int\limits_{S_{1\,\alpha}S^*_{1\,\alpha}} L(x,\ \theta_1)\, dx\, + \int\limits_{\overline{S}_{1\, \alpha} S^*_{1\, \alpha}} L(x,\ \theta_1)\, dx =\]
\[=\int\limits_{S_{1\,\alpha}S^*_{1\,\alpha}} T(x) L(x,\ \theta_0)\, dx + \int\limits_{\overline{S}_{1\, \alpha} S^*_{1\, \alpha}} T(x) L(x,\ \theta_0)\, dx\]
Чуток пошаманим с выведенными формулами:
\begin{equation*}
    \begin{aligned}
        \beta\left( \theta_1,\ S_{1\, \alpha} \right) - \beta\left( \theta_1,\ S_{1\, \alpha}^* \right) =& \left( \int\limits_{S_{1\, \alpha} S_{1\, \alpha}^*} T(x) L(x,\ \theta_0)\, dx + \int\limits_{S_{1\, \alpha} \overline{S}_{1\, \alpha}^*} T(x)L(x,\ \theta_0)\, dx \right) -\\
        &- \left( \int\limits_{S_{1\, \alpha}S_{1\, \alpha}^*} T(x)L(x,\ \theta_0)\, dx\, + \int\limits_{\overline{S}_{1\, \alpha} S^*_{1\, \alpha}} T(x)L(x,\ \theta_0)\, dx \right) \Rightarrow\\
        \Rightarrow\beta\left( \theta_1,\ S_{1\, \alpha} \right) - \beta\left( \theta_1,\ S_{1\, \alpha}^* \right)=&\int\limits_{S_{1\, \alpha} \overline{S}^*_{1\, \alpha}} \underset{< c_{\alpha}}{\underbrace{T(x)}} L(x,\ \theta_0)\, dx - \int\limits_{S^*_{1\, \alpha} \overline{S}_{1\, \alpha}} \underset{\geq c_{\alpha}}{\underbrace{T(x)}} L(x,\ \theta_0)\, dx
    \end{aligned}
\end{equation*}
Теперь можно составить равенство:
\[\beta(\theta_1,\ S_{1\, \alpha}) =\, \beta(\theta_1,\ S^*_{1\, \alpha}) + \int\limits_{S_{1\, \alpha} \overline{S}^*_{1\, \alpha}} \underset{< c_{\alpha}}{\underbrace{T(x)}} L(x,\ \theta_0)\, dx - \int\limits_{S^*_{1\, \alpha} \overline{S}_{1\, \alpha}} \underset{\geq c_{\alpha}}{\underbrace{T(x)}} L(x,\ \theta_0)\, dx\]
Правый интеграл содержит область $S_{1\, \alpha}^*$, по заданию это множество таких точек, в которых $T(x) \geq c_{\alpha}$.\\
Левый интеграл, наоборот, содержит $\overline{S}_{1\, \alpha}^*$, то есть все точки, в которых $T(x) < c_{\alpha}$.\\
Значит будет справедливо неравенство:
\[\beta(\theta_1,\ S_{1\, \alpha}) < \beta(\theta_1,\ S^*_{1\, \alpha}) + c_{\alpha}\left( \int\limits_{S_{1\, \alpha} \overline{S}^*_{1\, \alpha}} L(x,\ \theta_0)\, dx - \int\limits_{S^*_{1\, \alpha} \overline{S}_{1\, \alpha}} L(x,\ \theta_0)\, dx \right)\]
Вероятность попадания в критическую область должна быть равна $\alpha$, тогда верно:
\[\alpha = \int\limits_{S_{1\, \alpha}} L(x,\ \theta_0)\, dx = \int\limits_{S^*_{1\, \alpha}} L(x,\ \theta_0)\, dx\]
При этом
\begin{equation*}
    \begin{aligned}
        \int\limits_{S_{1\, \alpha}} L(x,\ \theta_0)\, dx &= \int\limits_{S_{1\, \alpha} S^*_{1\, \alpha}} L(x,\ \theta_0)\, dx + \int\limits_{S_{1\, \alpha} \overline{S}_{1\, \alpha}^*} L(x,\ \theta_0)\, dx\\
        \int\limits_{S^*_{1\, \alpha}} L(x,\ \theta_0)\, dx &= \int\limits_{ S^*_{1\, \alpha} S_{1\, \alpha}} L(x,\ \theta_0)\, dx + \int\limits_{S^*_{1\, \alpha} \overline{S}_{1\, \alpha}} L(x,\ \theta_0)\, dx\\
        \int\limits_{S_{1\, \alpha}} L(x,\ \theta_0)\, dx - \int\limits_{S^*_{1\, \alpha}} L(x,\ \theta_0)\, dx &= \int\limits_{S_{1\, \alpha} \overline{S}_{1\, \alpha}^*} L(x,\ \theta_0)\, dx\, - \int\limits_{S^*_{1\, \alpha} \overline{S}_{1\, \alpha}} L(x,\ \theta_0)\, dx = \alpha - \alpha = 0\\
    \end{aligned}
\end{equation*}
Тогда в ранее записанном неравенстве:
\[c_{\alpha}\left(\int\limits_{S_{1\, \alpha} \overline{S}_{1\, \alpha}^*} L(x,\ \theta_0)\, dx - \int\limits_{S^*_{1\, \alpha} \overline{S}_{1\, \alpha}} L(x,\ \theta_0)\, dx\right) = 0 \Rightarrow\]
\[\Rightarrow \beta(\theta_1,\ S_{1\, \alpha}) < \beta(\theta_1,\ S_{1\, \alpha}^*) + 0 \Rightarrow \beta(\theta_1,\ S_{1\, \alpha}) < \beta(\theta_1,\ S_{1\, \alpha}^*)\]
То есть всякая критическая область, отличная от $S^*_{1\, \alpha}$, будет менее мощной.

\subsection*{Задача}
$X_1,\dots,\ X_n \sim N(m, \sigma^2)$, дисперсия известна. Построить наиболее мощный критерий для проверки $H_0: m = m_0$ против $H_1: m = m_1 > m_0$
\subsubsection*{Решение (моё)}
По лемме Неймана-Пирсона критическая область необходимого нам критерия должна выглядеть так:
\[S_{1\, \alpha}^* = \{(x_1,\dots,\ x_n)\ |\ T(x) \geq c_{\alpha}\},\ T(x) = \frac{L(x,\ m_1)}{L(x,\ m_0)} \geq c_{\alpha},\ P_{m_0} \left( T(x) \geq c_{\alpha} \right) = \alpha\]
\[L(x,\ m_1) = \prod_{i = 1}^{n} f(x_i,\ m_1) = \left( \frac{1}{\sqrt{2\pi} \sigma} \right)^n e^{-\sum_{i = 1}^{n}\frac{(x_i - m_1)^2}{2\sigma^2}}\]
\[L(x,\ m_0) = \prod_{i = 1}^{n} f(x_i,\ m_0) = \left( \frac{1}{\sqrt{2\pi} \sigma} \right)^n e^{-\sum_{i = 1}^{n}\frac{(x_i - m_0)^2}{2\sigma^2}}\]
\[\frac{L(x,\ m_1)}{L(x,\ m_0)} = e^{\sum_{i = 1}^{n} \frac{(x_i - m_0)^2 - (x_i - m_1)^2}{2\sigma^2}} = e^{\sum_{i = 1}^{n} \frac{(m_0 - m_1)(2x_i - m_1 - m_2)}{2\sigma^2}}\]
Хотим найти такое $c_{\alpha}$, что $P\left( T(x) \geq c_{\alpha} \right) = \alpha$, то есть хотим найти:
\[F_{T(x)}(c_{\alpha}) = \alpha\Rightarrow \int\limits_{-\infty}^{c_{\alpha}} e^{\sum_{i = 1}^{n} \frac{(m_0 - m_1)(2x_i - m_1 - m_2)}{2\sigma^2}}\, dx = \alpha\] 
\subsubsection*{Ответ с лекции}
$S^*_{1\, \alpha} \{(x_1,\dots,\ x_n)\ | \ \overline{X} \geq m_0 + \frac{Z_{1 - \alpha}\sqrt{n}}{\sigma}\} = \{(x_1,\dots,\ x_n)\ |\ \frac{(\overline{X} - m_0)\sqrt{n}}{\sigma} \geq Z_{1 - \alpha}\}$
\subsection*{Задача}
Для проверки гипотезы $H_0: m = m_0$
\[T(x) = \frac{(\overline{X} - m_0)\sqrt{n}}{\sigma}\Rightarrow T(x)\big|_{H_0: m = m_0} \sim N(0,\ 1)\]
Против гипотезы $H_1: m > m_0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-2, 0) -- (1, 0);
    \draw[color=red] (1, 0) -- (2, 0);
    \draw (1, 0.15) -- (1, -0.15);
    \node at (1, -0.3) (a) {$Z_{1 - \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_2: m < m_0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-1, 0) -- (2, 0);
    \draw[color=red] (-2, 0) -- (-1, 0);
    \draw (-1, -0.15) -- (-1, 0.15);
    \node at (-1, -0.3) (a) {$Z_{\alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_3: m \neq m_0$
\[
\begin{tikzpicture}[scale = 2]
    \draw[color=red] (-2, 0) -- (-0.5, 0);
    \draw[color=red] (0.5, 0) -- (2, 0);
    \draw[color=green] (-0.5, 0) -- (0.5, 0);
    \draw (0.5, -0.15) -- (0.5, 0.15);
    \draw (-0.5, -0.15) -- (-0.5, 0.15);
    \node at(-0.5, -0.3) (a) {$Z_{\alpha/2}$};
    \node at(0.5, -0.3) (a) {$Z_{1 - \alpha/2}$};
\end{tikzpicture}
\]
\textbf{Пояснение:} на рисунках зелёным обозначена доверительная область, красным обозначена критическая область.
\subsection*{Задача}
Снова гауссовская выборка, но дисперсия неизвестна. Хотим проверить гипотезу $H_0: m = m_0$. Тогда нужно поменять статистику на:
\[T(x) = \frac{(\overline{X} - m_0) \sqrt{n}}{\tilde{S}} = \frac{(\overline{X} - m_0)\sqrt{n - 1}}{S}\]
\[T(x) \big|_{H_0} \sim t(n - 1)\]
Против гипотезы $H_1: m > m_0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-2, 0) -- (1, 0);
    \draw[color=red] (1, 0) -- (2, 0);
    \draw (1, 0.15) -- (1, -0.15);
    \node at (1, -0.3) (a) {$t_{n - 1,\ 1 - \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_2: m < m_0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-1, 0) -- (2, 0);
    \draw[color=red] (-2, 0) -- (-1, 0);
    \draw (-1, -0.15) -- (-1, 0.15);
    \node at (-1, -0.3) (a) {$t_{n - 1,\ \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_3: m \neq m_0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=red] (-2, 0) -- (-0.5, 0);
    \draw[color=red] (0.5, 0) -- (2, 0);
    \draw[color=green] (-0.5, 0) -- (0.5, 0);
    \draw (0.5, -0.15) -- (0.5, 0.15);
    \draw (-0.5, -0.15) -- (-0.5, 0.15);
    \node at(-0.5, -0.3) (a) {$t_{n - 1,\ \alpha/2}$};
    \node at(0.5, -0.3) (a) {$t_{n - 1,\ 1 - \alpha/2}$};
\end{tikzpicture}\]
Та же самая идея, только разделение идёт по квантилям распределения Стьюдента.
\subsection*{Задача}
Теперь строим критерий для оценки дисперсии при известном математическом ожидании. Проверяем гипотезу $H_0: \sigma = \sigma_0$:
\[T(x) = \frac{\sum_{i = 1}^{n} (x_i - m)^2}{\sigma_0^2}\]
\[T(x) \big|_{H_0: \sigma = \sigma_0} \sim \chi^2(n)\]
Против гипотезы $H_1: \sigma < \sigma_0$
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[red] (0, 0) -- (0.5, 0);
    \draw[green] (0.5, 0) -- (2, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (0.5, -0.15) -- ++(0, 0.3);
    \node at (0.5, -0.3) (a) {$\chi^2_{n, \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_2: \sigma > \sigma_0$
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[green] (0, 0) -- (1, 0);
    \draw[red] (1, 0) -- (2, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (1, -0.15) -- ++(0, 0.3);
    \node at (1, -0.3) (a) {$\chi^2_{n, 1 - \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_3: \sigma \neq \sigma_0$
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[green] (0, 0) -- (0.5, 0);
    \draw[green] (1.5, 0) -- ++(0.5, 0);
    \draw[red] (0.5, 0) -- (1.5, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (0.5, -0.15) -- ++(0, 0.3);
    \draw (1.5, -0.15) -- ++(0, 0.3);
    \node at (0.5, -0.3) (a) {$\chi^2_{n, \alpha}$};
    \node at (1.5, -0.3) (a) {$\chi^2_{n, 1 - \alpha}$};
\end{tikzpicture}\]
\subsection*{Задача}
Если математическое ожидание неизвестно:
\[T(x) = \frac{\sum_{i = 1}^{n} (x_i - \overline{X})^2}{\sigma_0^2}\]
\[T(x) \big|_{H_0: \sigma = \sigma_0} \sim \chi^2(n - 1)\]
Против гипотезы $H_1: \sigma < \sigma_0$
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[red] (0, 0) -- (0.5, 0);
    \draw[green] (0.5, 0) -- (2, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (0.5, -0.15) -- ++(0, 0.3);
    \node at (0.5, -0.3) (a) {$\chi^2_{n - 1, \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_2: \sigma > \sigma_0$
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[green] (0, 0) -- (1, 0);
    \draw[red] (1, 0) -- (2, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (1, -0.15) -- ++(0, 0.3);
    \node at (1, -0.3) (a) {$\chi^2_{n - 1, 1 - \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_3: \sigma \neq \sigma_0$
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[green] (0, 0) -- (0.5, 0);
    \draw[green] (1.5, 0) -- ++(0.5, 0);
    \draw[red] (0.5, 0) -- (1.5, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (0.5, -0.15) -- ++(0, 0.3);
    \draw (1.5, -0.15) -- ++(0, 0.3);
    \node at (0.5, -0.3) (a) {$\chi^2_{n - 1, \alpha}$};
    \node at (1.5, -0.3) (a) {$\chi^2_{n - 1, 1 - \alpha}$};
\end{tikzpicture}\]
\subsection*{Проверка гипотез о распределении случайных величин}
\section*{Критерий Колмогорова (КАКОЙ ЖЕ ОН КРУТОЙ)}
$X_1,\dots,\ X_n \sim F_{\xi} (x,\ \theta_0) = F_0(x),\ \theta_0$ известна. Проверяем гипотезу $H_0: \xi \sim F_0(x)$\\
Колмогоров предложил считать $D_n = \max\limits_{1 \leq i \leq n} \left| \hat F_n (x_i) - F_0(x_i) \right|$.\\
Если $n\to \infty$ (начиная с 20 уже хорошая апроксимация) и при условии верности $H_0$ получаем 
\[\sqrt{n} D_n \sim K(t)\]
Функция распределения Колмогорова
\[K(t) = \sum_{j = -\infty}^{+\infty} (-1)^j \exp\{-j^2 t^2\}\]
\subsection*{Критерий хи-квадрат}
$X_1,\dots,\ X_n \sim F_{\xi}(x,\ \theta_0) = F_0(x)$, $\theta_0$ знаем. Проверяем гипотезу $H_0: \xi \sim F_0(x)$.\\
Делим $\mathbb{R}^1$ на $l + 2$ интервала, где $S_0 = -\infty,\ S_{l + 1} = +\infty$ тогда $\hat p_k = \frac{n_k}{n},\ p_k^{(0)} = F_0(S_k) - F_0(S_{k - 1})$, где $k = \overline{1,\ l + 1}$.\\
Здесь возникает $\hat \chi^2 = \sum_{k = 1}^{l + 1} \frac{n}{p^{(0)}_k} \left( \hat p_k - p_k^{(0)}\right)^2$
\subsection*{Утверждение}
Если $0 < p_k^{(0)} < 1$ для $\forall k = \overline{1,\ l + 1},\ n \to \infty$ и справедлива $H_0$, то 
\[\hat \chi^2 \sim \chi^2 (l)\] 
Тогда график будет выглядеть так
\[\begin{tikzpicture}
    \draw[green] (-1, 0) -- (0, 0);
    \draw[green] (0, 0) -- (1, 0);
    \draw[red] (1, 0) -- (2, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.5) (a) {$0$};
    \draw (1, -0.15) -- ++(0, 0.3);
    \node at (1, -0.5) (a) {$\chi^2_{l, 1 - \alpha}$};
\end{tikzpicture}\]
\subsection*{Задача}
Проверяем теории из биологии
\[\begin{tabular}{|c|c|c|c|}
    \hline
   & $p_{k}^{(0)}$ & $n_k$ & $\hat p_k = \frac{n_k}{n}$\\\hline
AB & $\frac{9}{16}$ & 315 & 0.556\\    \hline
Ab & $\frac{3}{16}$ & 108 & 0.194\\    \hline
aB & $\frac{3}{16}$ & 101 & 0.182\\    \hline
ab & $\frac{1}{16}$ & 32  & 0.058\\    \hline
\end{tabular}\]
Теперь проверим $H_0: \vec{p}^{(0)} = \left( \frac{9}{16},\ \frac{3}{16},\ \frac{3}{16},\ \frac{1}{16} \right)$\\
Применяя критерий хи-квадрат:
\[\hat \chi^2 = 0.49,\ \text{посчитали за кадром}\]
\[\hat \chi^2 \big|_{H_0} \sim \chi^2 (3)\]
Тогда при параметрах $\alpha = 0.05,\ \chi^2_{3,\ 0.95} = 7.81\Rightarrow$ наш результат лежит в доверительной области.

\begin{center}
    \bf Лекция 28 февраля
\end{center}

\section*{Критерий хи-квадрат Пирсона}
Имеется выборка $X_1,\dots,\ X_n \sim F_{\xi}(x,\ \theta),\ \theta\in \Theta \subset \mathbb{R}^n$.\\
Проверяем гипотезу $H_0: \xi \sim F^{0}_{\xi}(x,\ \theta)$ (здесь использован верхний индекс для указания на какое-то конкретное распределение).
\begin{enumerate}
    \item Оценим вектор параметров $\theta = (\theta_1,\dots,\ \theta_m)$ по методу максимального правдоподобия.
    \item Разбиваем $\mathbb{R}^1$ на $(l + 1)$ непересекающийся интервал.
    \[\begin{tikzpicture}[scale=2]
        \draw (-3, 0) -- (3, 0);
        \draw (-2, 0.1) -- ++(0, -0.2);
        \node at (-2.5, 0.15) (1) {$\Delta_0$};
        \node at (-2, -0.2) (1) {$x_{(1)} = s_1$};
        \draw (-1, 0.1) -- ++(0, -0.2);
        \draw (0, 0.1) -- ++(0, -0.2);
        \draw (1, 0.1) -- ++(0, -0.2);
        \node at (-1, -0.2) (1) {$s_2$};
        \draw (2, 0.1) -- ++(0, -0.2);
        \node at (2, -0.2) (1) {$x_{(n)} = s_l$};
        \node at (2.5, 0.15) (1) {$\Delta_l$};
    \end{tikzpicture}\]
    \item Введём следующие обозначения: \\
    $\forall k \in [1,\ l - 1] \cap \mathbb{Z}\quad \hat p_k = \frac{n_k}{n}\\
    \forall k \in [0,\ l] \cap \mathbb{Z}\quad p_k^{(0)}\big( \hat \theta \big) = P_{H_0}\left( \xi \in \Delta_k \right),\ \text{(вероятность $\xi$ попасть в $k$-ый интервал при условии $H_0$)}\\
    p_k^{(0)}\big( \hat \theta \big) = F\left( s_{k + 1},\ \hat \theta \right) - F\left( s_{k},\ \hat \theta \right)$
\end{enumerate}
Тогда справедливо
\[\hat \chi^2 = \sum_{k = 0}^{l} \frac{n}{p_{k}^{(0)} (\hat \theta)} \left( \hat p_k - \hat p_k^{(0)} (\hat \theta) \right)^2 = np_0^{(0)}(\hat \theta) + \sum_{k = 1}^{l - 1} \frac{n}{p_{k}^{(0)} (\hat \theta)} \left( \hat p_k - \hat p_k^{(0)} (\hat \theta) \right)^2 + np_l^{(0)}(\hat \theta)\]
\subsection*{Утверждение}
При $n\to \infty$, $p_k^{(0)} > 0, \displaystyle \sum_{k = 0}^{l} p_{k}^{(0)} = 1$ и соблюдении некоторых условий регулярности (про дифференцируемость и существование вторых производных) выполняется
\[\hat \chi^2\big|_{H_0} \sim \chi^2 (l + 1 - 1 - m)\]
Здесь $l + 1$ --- количество интервалов, а $m$ --- количество оцененных параметров.
Доверительным интервалом будет $(0,\ \chi^2_{1 - \alpha,\ l - m})$ 
\subsection*{Определение}
Выборки $X_1,\dots,\ X_m \sim F_{x}(t)$ и $Y_1,\dots,\ Y_m \sim F_y(t)$ называются \underline{однородными}, если
\[\forall t \in \real^1\quad F_x(t) \sim F_y(t)\]
Для доказательства однородности выборок следует проверять гипотезу $H_0: \forall t\in \real^1\quad F_x(t) = F_y(t)$
\subsection*{Пример}
Имеется две выборки $X_1,\dots,\ X_m \sim F(t)$ и $Y_1,\dots,\ Y_n \sim F(t - \theta)$. Пусть $|EX| < \infty$, тогда
\[EY_1 = \int\limits_{-\infty}^{+\infty} tf_y(t)\, dt = \int\limits_{-\infty}^{+\infty} tf_x(t - \theta)\, dt = \left< \begin{aligned}
    &t - \theta = z\\
    &t = z + \theta 
\end{aligned} \right> = \int\limits_{-\infty}^{+\infty} (z + \theta) f_x(z),\ dz = \]
\[=\underset{EX}{\underbrace{\int\limits_{-\infty}^{+\infty} z f_x(z)\, dz}} + \theta \underset{1}{\underbrace{\int\limits_{-\infty}^{+\infty} f_x(z)\, dz}} = EX + \theta\]
Тогда для проверки однородности могут быть использованы гипотезы:
\[H_0: \theta = m_y - m_x = 0,\ \text{против } \left[ \begin{aligned}
    H_1: \theta < 0\ (m_y < m_x)\\
    H_2: \theta > 0\ (m_y > m_x)\\
    H_3: \theta \neq 0\ (m_y \neq m_x)\\
\end{aligned} \right.\]
\subsection*{Критерий Стьюдента}
Есть две выборки $X_1,\dots,\ X_m \sim N(m_x,\ \sigma^2)$ и $Y_1,\dots,\ Y_n \sim N(m_y,\ \sigma^2)$. Выборки независимы и имеют одинаковые (но неизвестные нам) дисперсии.\\
Тогда для проверки гипотезы $H_0: m_y - m_x = 0$ подойдёт статистика:
\[T(x,\ y) = \frac{\overline{Y} - \overline{X}}{S \sqrt{\frac{1}{n} + \frac{1}{m}}}\]
Здесь $S^2 = \frac{\sum_{i = 1}^{m} \left( x_i - \overline{X} \right)^2 + \sum_{i = 1}^{n} \left( y_i - \overline{Y} \right)^2}{m + n - 2}$. При верности гипотезы $H_0$ получаем
\[T(x,\ y)\big|_{H_0} \sim t(n + m - 2)\]
Против гипотезы $H_1: \theta < 0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-1, 0) -- (2, 0);
    \draw[color=red] (-2, 0) -- (-1, 0);
    \draw (-1, -0.15) -- (-1, 0.15);
    \node at (-1, -0.3) (a) {$t_{m + n - 2,\ \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_2: \theta > 0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-2, 0) -- (1, 0);
    \draw[color=red] (1, 0) -- (2, 0);
    \draw (1, 0.15) -- (1, -0.15);
    \node at (1, -0.3) (a) {$t_{m + n - 2,\ 1 - \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_3: \theta \neq 0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=red] (-2, 0) -- (-1, 0);
    \draw[color=red] (1, 0) -- (2, 0);
    \draw[color=green] (-1, 0) -- (1, 0);
    \draw (1, -0.15) -- (1, 0.15);
    \draw (-1, -0.15) -- (-1, 0.15);
    \node at(-1, -0.3) (a) {$t_{m + n - 2,\ \alpha/2}$};
    \node at(1, -0.3) (a) {$t_{m + n - 2,\ 1 - \alpha/2}$};
\end{tikzpicture}\]
\section*{Ранговые критерии}
\subsection*{Определение}
\underline{Рангом элемента выборки} называется его номер в вариационном ряду:
\[R(x_{(k)}) = k\]
Процедура определения рангов элементов выборки называется \underline{ранжированием}.
\subsection*{Определение}
\underline{Связкой размера $n$} называют $n$ совпадающих элементов выборки.\\
Если связке размера $m$ предшествует $k$ элементов, то все элементы связки получают один ранг, равный
\[\frac{1}{m} \sum_{i = k + 1}^{m + k} i\]
\subsection*{Ранговый критерий Вилкоксона (1945)}
Предполагается $X_1,\dots,\ X_m \sim F(t)$ и $Y_1,\dots,\ Y_n \sim F(t - \theta)$. Выборки независимы, $F(t)$ --- непрерывное распределение. Проверяем гипотезу $H_0: \theta = 0$.\\
Интуитивно понятно, что в случае $\theta \ll 0$ (математическое ожидание $Y$ сильно меньше, чем у $X$) элементы в вариационном ряду располагаются так:
\[y_{(1)},\dots,\ y_{(n)} x_{(1)},\dots,\ x_{(m)}\]
И в случае $\theta \gg 0$:
\[x_{(1)},\dots,\ x_{(m)} y_{(1)},\dots,\ y_{(n)}\]
Для проверки критерия введём следующую статистику:
\[W_{m,\ n} = \sum_{i = 1}^{n} R_{i},\text{ где }R_i-\text{ранг $Y_{i}$ в объединённой выборке}\]
Тогда для случая $\theta \ll 0$
\[\min W_{m,\ n} = \sum_{i = 1}^{n} R_i = (n + 1)\frac{n}{2}\]
Для случая $\theta \gg 0$
\[\max W_{m,\ n} = \sum_{i = 1}^{n} R_i = (n + 2m + 1)\frac{n}{2}\]
Если $\theta = 0$, то выборка должна быть перемешана, тогда для статистики справедливо.
\[EW_{m,\ n}\big|_{H_0} = (n + m + 1)\frac{n}{2},\ \dev W_{m,\ n} = \frac{mn}{12}(m + n + 1)\]
\begin{center}
    \bf Лекция 7 марта
\end{center}
Разбираем пример на применение критерия Вилкоксона.\\
$X_1,\dots,\ X_m \sim F_{x}(t)$ и $Y_1,\dots,\ Y_n \sim F_{y}(t - \theta)$. Проверяем гипотезу $H_0: \theta = 0$.
\[W_{m,\ n} = \sum_{i = 1}^{n} R_i\]
Пусть $m = 4,\ n = 2$, тогда есть $C^2_6 = 15$ способов расставить $y$. Пусть $(R_1,\ R_2) = (r_1,\ r_2)$, тогда: 
\[\begin{tabular}{|c|c|c|}
    \hline
    $(r_1,\ r_2)$ & $W_{4,\ 2}$ & $P_{H_0}\big( (R_1,\ R_2) = (r_1,\ r_2) \big)$\\
    \hline
    $ (1,\ 2) $ & 3 & $\frac{1}{15}$\\
    \hline
    $ (1,\ 3) $ & 4 & $\frac{1}{15}$\\ 
    \hline
    $ (1,\ 4) $ & 5 & $\frac{1}{15}$\\ 
    \hline
    $ (1,\ 5) $ & 6 & $\frac{1}{15}$\\ 
    \hline
    $ (1,\ 6) $ & 7 & $\frac{1}{15}$\\ 
    \hline
    $ (2,\ 3) $ & 5 & $\frac{1}{15}$\\ 
    \hline
    $ (2,\ 4) $ & 6 & $\frac{1}{15}$\\ 
    \hline
    $ (2,\ 5) $ & 7 & $\frac{1}{15}$\\ 
    \hline
    $ (2,\ 6) $ & 8 & $\frac{1}{15}$\\ 
    \hline
    $ (3,\ 4) $ & 7 & $\frac{1}{15}$\\ 
    \hline
    $ (3,\ 5) $ & 8 & $\frac{1}{15}$\\ 
    \hline
    $ (3,\ 6) $ & 9 & $\frac{1}{15}$\\ 
    \hline
    $ (4,\ 5) $ & 9 & $\frac{1}{15}$\\ 
    \hline
    $ (4,\ 6) $ & 10 & $\frac{1}{15}$\\ 
    \hline
    $ (5,\ 6) $ & 11 & $\frac{1}{15}$\\ 
    \hline
\end{tabular}\]
Теперь можем составить таблицу
\[\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    $W_{4,\ 2}$ & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11\\
    \hline
    $P$ & $\frac{1}{15}$ & $\frac{1}{15}$ & $\frac{2}{15}$ & $\frac{2}{15}$ & $\frac{3}{15}$ & $\frac{2}{15}$ & $\frac{2}{15}$ & $\frac{1}{15}$ & $\frac{1}{15}$\\
    \hline
\end{tabular}\]
Получается симметричное распределение, его функция распределения в некоторых точках:
\[F_{W}(3) = \frac{1}{15},\ F_{W}(4) = \frac{2}{15}\]
$E W_{m,\ n} = (m + n + 1)\frac{n}{2}\Rightarrow EW_{4,\ 2} = 7$\\
Распределение дискретное, поэтому квантиль считается так 
\[Z_{\beta} = \min\{x\ \big|\ F(x) \geq \beta\}\]
Если $\min(m,\ n) \to\infty$, то 
\[W^* = \left.\frac{W - EW_{m,\ n}}{\sqrt{\dev W_{m,\ n}}}\right|_{H_0} \to N(0,\ 1)\]
Поправка на наличие связок. Имеется $l$ связок и $t_k$ --- размер $k$-ой связки ($k = \overline{1,\ l}$). Тогда
\[\tilde \dev W_{m,\ n} = \dev W_{m,\ n} - \frac{mn\sum_{i = 1}^{l} t_k(t_k^2 - 1)}{12 N(N - 1)},\ \text{где $N = m + n$}\]
Далее идёт 10 минут обсуждения плюсов данного метода.
\subsection*{Проверка гипотезы об однородности против гипотезы о растяжении (сжатии)}
$X_1,\dots,\ X_m \sim F(t - \mu)$ и $Y_1,\dots,\ Y_n \sim F\left( \frac{t - \mu}{\Delta} \right),\ \Delta > 0$\\
Если $\displaystyle \int\limits_{-\infty}^{+\infty} tf(t)\, dt = 0$ и $\exists \dev X$, то\\
$EX = \displaystyle \int\limits_{-\infty}^{+\infty} tf(t - \mu)\, dt = \left< z = t - \mu \right> = \int\limits_{-\infty}^{+\infty} (z + \mu) t(z)\, dz = \mu\\
\dev X = \int\limits_{-\infty}^{+\infty} (t - \mu)^2 f(t - \mu)\, dt = \int\limits_{-\infty}^{+\infty} z^2\, f(z)\, dz\\
\dev Y = \int\limits_{-\infty}^{+\infty} (t - \mu)^2 \frac{1}{\Delta} f\left( \frac{t - \mu}{\Delta} \right) \, dt = \left< \begin{aligned}
    & z =\frac{t - \mu}{\Delta}\\
    & dz = \frac{1}{\Delta} dt
\end{aligned} \right> \int\limits_{-\infty}^{+\infty} \Delta^2 z^2 f(z)\, dz = \Delta^2 \dev X\Rightarrow \frac{\dev Y}{\dev X} = \Delta^2$
\subsection*{Критерий Фишера}
$X_1,\dots,\ X_m \sim N(m_1,\ \sigma_1^2),\ Y_1,\dots,\ Y_n \sim N(m_2,\ \sigma_2^2)$\\
Случайные величины независимы, параметры неизвестны. Проверяем гипотезу $H_0: \sigma_1^2 = \sigma_2^2$
\[T(x,\ y) = \frac{\frac{1}{m - 1} \sum_{i = 1}^{m} {\left(x_i - \overline{X}\right)}^2}{\frac{1}{n - 1} \sum_{i = 1}^{n} {\left(y_i - \overline{Y}\right)}^2} \Bigg|_{H_0} \sim F(m - 1,\ n - 1),\ \text{распределение Фишера}\]
$\xi \sim F(m,\ n)\Rightarrow \frac{1}{\xi} \sim F(n,\ m)$. Тогда для квантилей справедливо:
\[z_{\beta} - \text{ квантиль уровня $\beta$ распределения $F(m,\ n)$}, \frac{1}{z_{\beta}} - \text{квантиль уровня $(1 - \beta)$ распределения $F(n,\ m)$}\]
$\beta = P\left(\xi \leq z_{\beta}\right) = P\left(\frac{1}{\xi} \geq \frac{1}{z_{\beta}}\right) = 1 - \underset{=1 - \beta}{\underbrace{P\left(\frac{1}{\xi} \leq \frac{1}{z_{\beta}}\right)}}$\\
$H_1: \sigma_1^2 < \sigma_2^2$\\
$\tilde S_1^2 > \tilde S_2^2 \Rightarrow \text{принимаем $H_0$}$\\
$\tilde S_1^2 < \tilde S_2^2,\ T(x,\ y) = \frac{\tilde S_2^2}{\tilde S_1^2} \sim F(n - 1,\ m - 1)\Rightarrow$ на правом хвосте критическая область.\\
$H_2: \sigma_1^2 > \sigma^2_2\\
\tilde S_1^2 < \tilde S_2^2 \Rightarrow \text{принимаем $H_0$}\\
\tilde S_1^2 > \tilde S_2^2,\ T(x,\ y) = \frac{\tilde S_1^2}{\tilde S_2^2} \sim F(m - 1,\ n - 1)\Rightarrow$ снова на правом хвосте критическая область (поменяли числитель и знаменатель).\\
$H_3: \sigma_1 \neq \sigma_2$\\
$\tilde S_1^2 < \tilde S_2^2,\ T(x,\ y) = \frac{\tilde S_2^2}{\tilde S_1^2}\sim F(n - 1,\ m - 1)\Rightarrow$ критическая область на правом хвосте.\\
$\tilde S_1^2 > \tilde S_2^2,\ T(x,\ y) = \frac{\tilde S_1^2}{\tilde S_2^2} \sim F(m - 1,\ n - 1)\Rightarrow$ на правом хвосте критическая область.\\
\begin{center}
    \bf Лекция 14 марта
\end{center}
\subsection*{Критерий Ансари-Брэйли}
$X_1,\dots,\ X_m \sim F(t - \mu)$\\
$Y_1,\dots,\ Y_n \sim F\left( \frac{t - \mu}{\Delta} \right),\ \Delta > 0$
\subsubsection*{Предположения}
Выборки независимы, $F(\mu) = 0.5$\\
Проверяем гипотезу $H_0: \Delta = 1$
\subsubsection*{Замечание 1}
Если $\dev X < \infty$ и $\displaystyle\int\limits_{-\infty}^{+\infty} tf(t)\, dt = 0$, то $\Delta^2 = \frac{\dev Y}{\dev X}$\\
Если $\dev X = +\infty$, то $\begin{cases}
    \Delta < 1\Rightarrow \text{выборка $Y$ сжата относительно $X$}\\
    \Delta > 1\Rightarrow \text{выборка $Y$ растянута относительно $X$}
\end{cases}$
\subsubsection*{Замечание 2}
Если $X_1,\dots,\ X_m \sim F(t - \mu_1)$ и $Y_1,\dots,\ Y_n \sim F\left( \frac{t - \mu_2}{\Delta} \right)$ (то есть сдвиги $\mu_1,\ \mu_2$ различные), то рекомендуется найти выборочную медиану $\hat \mu_x$ и $\hat \mu_y$ и рассматривать выборки $x_1 - \hat \mu_x,\dots,\ x_m - \hat \mu_x$ и $y_1 - \hat \mu_y,\dots,\ y_n - \hat\mu_y$
\subsection*{Реально критерий Ансари-Брейли}
Вводим обозначение $m + n = N$, а также статистика:
\[A_{m,\ n} = \sum_{i = 1}^{N} \left( \frac{N + 1}{2} - \left| R_i - \frac{N + 1}{2} \right| \right)\]
Здесь $R_i$ --- ранг $X_i$ в объединённой выборке. По своей сути $\left| R_i - \frac{N + 1}{2} \right|$ есть расстояние до ближайшего конца выборки (если мы пронумеруем выборку в прямом и в обратном порядке, то каждый элемент получит минимальный из номеров).\\
Если $n + m \leq 20$, то существует таблица точных значений квантилей статистики $A$\\
Если $\min(m,\ n) \to \infty$, то
\[A^* = \left.\frac{A - EA_{m,\ n}}{\sqrt{\dev A_{m,\ n}}}\right|_{H_0} \sim N(0,\ 1)\]
Свойства данной статистики:
\[EA_{m,\ n} = \begin{cases}
    \frac{m(N + 2)}{4},\ & N \underset{2}{\equiv} 0\\
    \frac{m(N + 1)^2}{4N},\ & N \underset{2}{\equiv} 1\\
\end{cases}\]
\[\dev A_{m,\ n} = \begin{cases}
    \frac{mn(N + 2)(N - 2)}{48(N - 1)},\ & N \underset{2}{\equiv} 0\\
    \frac{mn(N^2 + 3)(N + 1)}{48 {N}^2},\   & N \underset{2}{\equiv} 1
\end{cases}\]
Если проверяем гипотезу $H_0: \Delta = 1$ (используем значение $A^*$)\\
Против гипотезы $H_1: \Delta < 1$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-1, 0) -- (2, 0);
    \draw[color=red] (-2, 0) -- (-1, 0);
    \draw (-1, -0.15) -- (-1, 0.15);
    \node at (-1, -0.3) (a) {$Z_{\alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_2: \Delta > 1$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-2, 0) -- (1, 0);
    \draw[color=red] (1, 0) -- (2, 0);
    \draw (1, 0.15) -- (1, -0.15);
    \node at (1, -0.3) (a) {$Z_{1 - \alpha}$};
\end{tikzpicture}\]
Против гипотезы $H_3: \Delta \neq 1$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=red] (-2, 0) -- (-1.5, 0);
    \draw[color=red] (1.5, 0) -- (2, 0);
    \draw[color=green] (-1.5, 0) -- (1.5, 0);
    \draw (1.5, -0.15) -- (1.5, 0.15);
    \draw (-1.5, -0.15) -- (-1.5, 0.15);
    \node at(-1.5, -0.3) (a) {$Z_{\alpha/2}$};
    \node at(1.5, -0.3) (a) {$Z_{1 - \alpha/2}$};
\end{tikzpicture}\]
\subsection*{MAD оценка (Medium Absolute Deviation)}
Оценка среднеквадратичного отклонения выборки с неизвестным распределением:
\[MAD = \underset{1 \leq i \leq n}{\operatorname{med}}\left| x_i - \underset{\hat mu}{\underbrace{\operatorname{med}(x_1,\dots,\ x_n)}} \right|\]
Это медиана модулей отклонения от выборочной медианы.
\subsection*{Критерий КОЛМОГОРОВА-Смирнова}
Даны две выборки $X_1,\dots,\ X_m \sim F(t)$ и $Y_1,\dots,\ Y_n \sim G(t)$.
\subsubsection*{Предположения}
Выборки независимые, $F(t),\ G(t)$ непрерывные.
\subsubsection*{Применение}
Проверяем гипотезу $H_0: \forall t\quad F(t) = G(t)$ против альтернативы общего вида: $H_1: \exists t\quad F(t) \neq G(t)$.\\
Оцениваем функции распределения с помощью эмпирических функций распределения.\\
Рассматривается статистика:
\[D_{m,\ n} = \max\limits_{1 \leq i \leq m + n} \left| \hat F_{m}(z_i) - \hat G_{n}(z_i) \right|\]
Здесь $z = (z_1,\dots,\ z_{m + n})$ --- объединённая выборка из $x_1,\dots,\ x_m,\ y_1,\dots,\ y_n$.\\
Если $m + n \leq 20$, то есть таблица с точными квантилями.\\
Если $m + n > 20$, тогда хорошей апроксимацией будет:
\[D_{m,\ n} \sim K(t),\ \text{(распределение Колмогорова)}\]
Тогда прямая разбивается на следующие области (отрицательные ):
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-2, 0) -- (1, 0);
    \draw[color=red] (1, 0) -- (2, 0);
    \draw (1, 0.15) -- (1, -0.15);
    \draw (-2, 0.15) -- ++(0, -0.3);
    \node at (-2, -0.3) (0) {0};
    \node at (1, -0.3) (a) {$k_{1 - \alpha}$};
\end{tikzpicture}\]
$k_{\alpha}$ --- квантиль Колмогорова уровня $\alpha$. Известная точка $k_{0.95} = 1.36$\\
Данный критерий наименее мощный среди всех упомянутых ранее, поскольку является более общим. Если понятно, с чем связана неоднородность выборок, то стоит применять более специализированные критерии.
\section*{Однофакторный дисперсионный анализ}
\subsection*{Определения}
Фактор --- какая-то переменная, которая по нашему мнению влияет на конечный результат.\\
Уровень фактора --- значение переменной фактора (в задаче их должно быть конечное число).\\
Отклик:
\[\begin{tabular}{|c|c|c|c|}
    \hline
    1 & 2 & $\dots$ & k\\
    \hline
    $x_{1\, 1}$ & $x_{1\, 2}$ & $\dots$ & $x_{1\, k}$\\
    \hline
    $x_{2\, 1}$ & $x_{2\, 2}$ & $\dots$ & $x_{2\, k}$\\
    \hline
    $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$\\
    \hline
    $x_{{n_1}\, 1}$ & $x_{{n_2}\, 2}$ & $\dots$ & $x_{{n_k}\, k}$\\
    \hline
\end{tabular}\]
Столбцы --- выборка, являющаяся результатом испытания с каким-то конкретным уровнем фактора (С ростом номера столбца переменная фактора растёт).
\[x_{i\, j} = \theta + \tau_j + \varepsilon_{i\, j}\]
$\varepsilon_{i\, j}$ --- независимое одинаковое распределение случайной величины с $E\varepsilon_{i\, j} = 0,\ \dev \varepsilon_{i\, j} = \sigma^2$ (дисперсия неизвестная).\\
$H_0: \tau_1 = \dots = \tau_k = 0$ против $ H_1: \exists i : \tau_i \neq 0 $.
\subsection*{Критерий Фишера}
\subsubsection*{Обозначения}
$N = n_1 + \dots + n_k\\
\overline{X}_N = \displaystyle \frac{1}{N} \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} x_{i\, j}\\$

\subsubsection*{Предположения}
$\varepsilon_{i\, j} \sim N(0,\ \sigma^2)$.
\subsubsection*{Формулировка}
\begin{equation*}
    \begin{aligned}
    SS_{\text{общ}} =& \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} {(x_{i\, j} - \overline{X}_N)}^2 = \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} \left( X_{i\, j} \pm \overline{X}_{\bullet\, j} - \overline{X}_N \right) = \underset{SS_{\text{случ.}}}{\underbrace{\sum_{j = 1}^{k} \sum_{i = 1}^{n_j} \left( x_{i\, j} - \overline{X}_{\bullet\, j} \right)^2}} + \underset{SS_{\text{ур. ф.}}}{\underbrace{\sum_{j = 1}^{k}\sum_{i = 1}^{n_j} (\overline{X}_{\bullet\, j} - \overline{X}_N)^2}} +\\
    & + \underset{ = 0}{\underbrace{\sum_{j = 1}^{k} \sum_{i = 1}^{n_j} \left( x_{i\, j} - \overline{X}_{\bullet\, j} \right)\left( \overline{X}_{\bullet\, j} - \overline{X}_N \right)}}
    \end{aligned}
\end{equation*}

\begin{center}
    \bf Лекция 21 марта
\end{center}
\subsubsection*{Напоминание}
\begin{equation*}
    \begin{aligned}
        SS_{\text{общ}} &= \displaystyle \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} {\left( x_{i\, j} - \overline{X}_{\bullet\, j} \right)}^2 + \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} {\left( \overline{X}_{\bullet\, j} - \overline{X}_{N} \right)}^2 = \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} {\left( x_{i\, j} - \overline{X}_{\bullet\, j} \right)}^2 + \sum_{j = 1}^{k} n_j {\left( \overline{X}_{\bullet\, j} - \overline{X}_{N} \right)}^2 =\\
        &= SS_{\text{случ.}} + SS_{\text{ур. ф.}}
    \end{aligned}
\end{equation*}
Из предположения о гауссовости $SS_{\text{случ.}}$:
\[\frac{SS_{\text{случ.}}}{\sigma^2} = \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} {\left( \frac{x_{i\, j} - \overline{X}_{\bullet\, j}}{\sigma} \right)}^2 \sim \chi^2(N - k)\] 
При справедливости гипотезы $H_0$:
\[\frac{SS_{\text{ур. ф.}}}{\sigma^2} = \sum_{j = 1}^{k} n_j {\left( \frac{\overline{X}_{\bullet\, j} - \overline{X}_{N}}{\sigma} \right)}^2 \bigg|_{H_0} \sim \chi^2(k - 1)\]
Тогда про следующую статистику известно:
\[\frac{\frac{SS_{\text{ур. ф.}}}{\sigma^2 (k - 1)} }{\frac{SS_{\text{случ.}}}{\sigma^2 (N - k)}} = \frac{\frac{SS_{\text{ур. ф.}}}{(k - 1)} }{\frac{SS_{\text{случ.}}}{(N - k)}} \sim F(k - 1,\ N - k)\]
Критической областью тогда будет $(F_{1 - \alpha,\ k - 1,\ N - k},\ +\infty)$.\\
Если $H_0$ отвергается, то
\begin{equation*}
    \begin{aligned}
        &x_{i\, j} = \theta_j + \varepsilon_{i\, j},\quad \varepsilon_{i,\ j} \sim N(0,\ \sigma^2)\\
        &\theta_{j} = \theta + \tau_j,\quad j = \overline{1,\ k}\\
        &\hat \theta_j = \overline{X}_{\bullet\, j}\\
        &\hat \theta_j \sim N\left(\theta_j,\ \frac{\sigma^2}{n_j}\right),\ \text{т. к.}\ \frac{(\overline{X}_{\bullet\, j} -\theta_j) \sqrt{n_j}}{\sigma} \sim N(0,\ 1)\\
        &\dev \hat \theta_j = \dev\left( \frac{1}{n_j} \sum_{i = 1}^{n_j} X_{i\, j} \right) = \frac{\sigma^2}{n_j}\\
        &\hat \sigma^2 = \frac{1}{N - k} \sum_{j = 1}^{k} \sum_{i = 1}^{n_j} {\left( x_{i\, j} - \overline{X}_{\bullet\, j}\right)}^2\\
        &\frac{\left( \overline{X}_{\bullet\, j} - \theta_j \right) \sqrt{n_j}}{\hat \sigma} \sim t(N - k) \Rightarrow\\
        &\Rightarrow P\left( t_{\alpha/2,\ N - k} < \frac{\left( \overline{X}_{\bullet\, j} - \theta_j \right) \sqrt{n_j}}{\hat \sigma} < t_{1 - \alpha/2,\ N - k} \right) = 1 - \alpha \Rightarrow\\
        &\Rightarrow P\left( \overline{X}_{\bullet\, j} - \frac{t_{1 - \alpha,\ N - k} \hat\sigma}{\sqrt{n_j}} < \theta_j < \overline{X}_{\bullet\, j} + \frac{t_{1 - \alpha,\ N - k} \hat\sigma}{\sqrt{n_j}} \right) = 1 - \alpha,\quad j = \overline{1,\ k}
    \end{aligned}
\end{equation*}
\subsection*{Определение}
Контрастом $\gamma$ параметров $\theta_j,\ j = \overline{1,\ k}$ в модели (*) называется:
\[\gamma = \sum_{j = 1}^{k} c_j \theta_j\]
где константы $c_j$ удовлетворяют $\displaystyle \sum_{j = 1}^{k} c_j = 0$. Обычно берут две константы равные $-1$ и $1$, остальные зануляют (в результате получаем, насколько контрастируют параметры конкретных столбов).
\subsubsection*{Определение}
Оценкой контраста считается:
\[\hat \gamma = \sum_{j = 1}^{k} c_j \hat \theta_j = \sum_{j = 1}^{k} c_j \overline{X}_{\bullet\, j}\]
Параметры оценки:
\begin{equation*}
    \begin{aligned}
        &\hat \gamma \sim N\left(\gamma,\ \sigma^2 \sum_{j = 1}^{k} \frac{c_j^2}{n_j}\right)\\
        &\dev \sum_{j = 1}^{k} c_j \overline{X}_{\bullet\, j} = \sum_{j = 1}^{k} c_j^2 \dev \overline{X}_{\bullet\, j} = \sigma^2 \sum_{j = 1}^k \frac{c_j^2}{n_j}\\
        & \frac{\hat \gamma - \gamma}{\sigma \sqrt{\sum_{j = 1}^k \frac{c_j^2}{n_j}}} \sim t(N - k)\\
        & P\left( t_{\alpha/2,\ N - k} < \frac{\hat \gamma - \gamma}{\hat \sigma \sqrt{\sum_{j = 1}^k \frac{c_j^2}{n_j}}} < t_{1 - \alpha/2,\ N- k } \right) = 1 - \alpha\\
        & P\left(\hat \gamma -  t_{1 - \alpha/2,\ N - k} \hat \sigma\sqrt{\sum_{j = 1}^{k} \frac{c_j^2}{n_j}} < \gamma < \hat \gamma +  t_{1 - \alpha/2,\ N - k} \hat \sigma\sqrt{\sum_{j = 1}^{k} \frac{c_j^2}{n_j}}\right) = 1 - \alpha
    \end{aligned}
\end{equation*}
\subsection*{Ранговый критерий Краскела-Уоллиса}
Имееются выборки $z_1 = (x_{1\, 1},\dots,\ x_{n_1\, 1}),\dots,\ z_k = (x_{1\, k},\dots,\ x_{n_k\, k})$
\subsubsection*{Предположение}
$\begin{cases}
    \text{Выборки независимы, как и элементы в них.}\\
    x_{1\, 1} \sim F(t - \theta_1),\dots,\ x_{i\, k} \sim F(t - \theta_k),\ i = \overline{1,\ n_j}\\
    \text{Распределение $F(t)$ непрерывное}
\end{cases}$
\subsubsection*{Гипотезы}
\[H_0: \theta_1 = \dots = \theta_k = \theta,\ \text{$\theta$ --- какое-то произовольное число для удобства обозначения}\]
\[H_1: \exists j: \theta_j \neq \theta\]
\subsubsection*{Обозначения}
$r_{i\, j}$ --- ранг $x_{i\, j}$ в объединённой выборке объёма $N = n_1 + \dots + n_k$\\
$\overline{r}_{\bullet\, j} = \frac{1}{n_j} \sum_{j = 1}^{n_j} r_{i\, j}$
\subsubsection*{Идея критерия}
Имеется таблица
\[\begin{tabular}{|c|c|c|c|}
    \hline
    1 & 2 & $\dots$ & $k$\\
    \hline
    $r_{1\, 1}$ & $r_{1\, 2}$ & $\dots$ & $r_{1\, k}$\\
    \hline
    $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$\\
    \hline
    $r_{n_1\, 1}$ & $r_{n_2\, 2}$ & $\dots$ & $r_{n_k\, k}$\\
    \hline
    \hline
    $\overline{r}_{\bullet\, 1}$ & $\overline{r}_{\bullet\, 2}$ & $\dots$ & $\overline{r}_{\bullet\, k}$\\
    \hline
\end{tabular}\]
Составим следующую статистику:
\[H = \frac{12}{N(N + 1)} \sum_{j = 1}^{k} n_j {\left( \overline{r}_{\bullet\, j} - \frac{N+1}{2} \right)}^2\]
Если $\min(n_1,\dots,\ n_k) \to \infty$:
\[H\Big|_{H_0} \sim \chi^2(k - 1)\]
Критерий выглядит вот так:
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[green] (0, 0) -- (1, 0);
    \draw[red] (1, 0) -- (2, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (1, -0.15) -- ++(0, 0.3);
    \node at (1, -0.3) (a) {$\chi^2_{1 - \alpha,\ k - 1}$};
\end{tikzpicture}\]
\subsubsection*{Замечание}
Всё написанное выше работает для выборки без связок. Если связки всё-таки есть, то нужен поправочный коэффициент.
\subsection*{Ранговый критерий Джонкхиера}
Условия совпадают с критерией Краскела-Уоллиса, но альтернативная гипотеза другая:
\subsubsection*{Гипотезы}
$H_0: \theta_1 = \dots = \theta_k = \theta$ против $H_1: \theta_1 \leq \theta_1 \leq \dots \leq \theta_k$, где хотя бы одно неравенство строгое. То есть предполагаем, что увеличение фактора ведёт к увеличению математического ожидания.
\subsubsection*{Идея критерия}
Введём функцию:
\[\varphi(y,\ z) = \begin{cases}
    1,\ &y < z\\
    0.5,\ & y = z\\
    0,\ & y > z
\end{cases}\]
И функцию:
\[U_{l,\ m} = \sum_{i = 1}^{n_l} \sum_{j = 1}^{n_m} \varphi(x_{i\, l},\ x_{j\, m})\]
Теперь возьмём статистику:
\[ J = \sum_{1 \leq l < m \leq k} U_{l,\ m} \]
Если $\min (n_1,\dots,\ n_k) \to \infty$:
\[ J^* = \frac{J - EJ}{\sqrt{\dev J}} \sim N(0,\ 1)\]
Параметры статистики $J$ (запоминать необязательно):
\begin{equation*}
    \begin{aligned}
        EJ &= \frac{1}{4}{\left( N^2 - \sum_{i = 1}^{k} n_i^2 \right)}\\
        \dev J &= \frac{1}{72} {\left( N^2 \left( 2N + 3 \right) - \sum_{i = 1}^{k} n_i^2 (2n_i + 3) \right)}
    \end{aligned}
\end{equation*}
Заметим, что при $k = 2$:
\[W = J + \frac{n_2(n_2 + 1)}{2}\]
Да и вообще статистика $J$ является статистикой Вилкоксона с каким-то смещением, то есть все его свойства наследуются.
\begin{center}
    \bf Лекция 4 апреля
\end{center}
\begin{center}
{\Large\textbf{Исследование зависимостей}}\\
Шкалы измерений
\end{center}
\begin{enumerate}
    \item Количественная (насколько одно больше другого, операция вычитания)
    \item Порядковая/ординальная (разделение на группы, операции больше или меньше)
    \item Номинальная (можем только проверять равенство элементов)
\end{enumerate}
Из более высокой шкалы можно перейти в более низкую, наоборот --- нельзя. Если сравниваются две метрики, измеряемые в разных шкалах, то нужно перевести их в одну (низшую из них).\\
Имеется две метрики:
\begin{equation*}
    \begin{aligned}
        & A: A_1,\dots,\ A_m\\
        & B: B_1,\dots,\ B_k
    \end{aligned}
\end{equation*}
Можем составить гипотезу независимости
\[H_0: \forall i,\ j\quad P(A = A_i,\ B = B_j) = \underset{=p_{i\, \bullet}}{\underbrace{P(A = A_i)}} \cdot \underset{=p_{\bullet\, j}}{\underbrace{P (B = B_j)}},\ \text{против}\ H_1: \exists (i,\ j)\quad P_{i\, j} \neq P(A = A_i) \cdot P(B = B_j)\]
\section*{Работаем с номинальной шкалой}
\subsection*{Определение}
\underline{Таблицей сопряжённости коэффициентов} называется:
\[\begin{tabular}{|c|c|c|c|c|}
    \hline
    $A\setminus B$ & $B_1$ & $\dots$ & $B_k$ &  \\
    \hline
    $A_1$ & $n_{1\, 1}$ & $\dots$ & $n_{1\, k}$ & $n_{1\, \bullet}$\\
    \hline
    $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\
    \hline
    $A_{m} $ & $n_{m\, 1}$ & $\dots$ & $n_{m\, k}$ & $n_{m\, \bullet}$\\
    \hline
     & $n_{\bullet\, 1}$ & $\dots$ & $n_{\bullet\, k}$ & $n$\\
     \hline
\end{tabular}\]
Теперь можем построить оценки:
\begin{equation*}
    \begin{aligned}
        &\hat p_{i\, j} = \frac{n_{i\, j}}{n} \xrightarrow[n \to \infty]{p} p_{i\, j}\\
        &\hat p_{i\, \bullet} = \frac{n_{i\, \bullet}}{n} \xrightarrow[n\to \infty]{p} p_{i\, \bullet}\\
        &\hat p_{\bullet\, j} = \frac{n_{\bullet\, j}}{n} \xrightarrow[n\to\infty]{p} p_{\bullet\, j}
    \end{aligned}
\end{equation*}
Для выполнения независимости хотелось бы:
\[p_{i\, j} = p_{i\, \bullet} \cdot p_{\bullet\, j} \Rightarrow \hat p_{i\, j} = \hat p_{i\, \bullet} \cdot \hat p_{\bullet\, j} \Rightarrow \frac{n_{i\, j}}{n} \approx \frac{n_{i\, \bullet}}{n} \cdot \frac{n_{\bullet\, j}}{n}\]
Можем составить статистику:
\[\hat \chi^2 = \sum_{i = 1}^{m} \sum_{j = 1}^{k} \frac{n \cdot {\left(n_{i\, j} - \frac{n_{i\, \bullet} n_{\bullet\, j}}{n}\right)}^2}{n_{i\, \bullet} n_{\bullet\, j}}\]
При справедливости $H_0$ выполняется
\[\hat \chi^2 \sim \chi^2\left( (m - 1)(k - 1) \right)\]
Критическая область справа.
\subsubsection*{Частный случай}
Для $m = k = 2$ верно:
\[\hat \chi^2 = \frac{n {(n_{11} n_{22} - n_{12} n_{21})}^2}{ n_{1\, \bullet}\, n_{2\, \bullet}\, n_{\bullet\, 1}\, n _{\bullet\, 2} }\]
\subsection*{Определение}
Для таблиц $2\times 2$:
\[\begin{tabular}{|c|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0 & \\
    \hline 
    1 & $a$ & $b$ & $a + b$\\
    \hline
    0 & $c$ & $d$ & $c + d$\\
    \hline
     & $a + c$ & $b + d$ & \\
     \hline
\end{tabular}\]
Определён \underline{коэффициент контингенции} $\Phi$:
\[\Phi = \frac{ad - bc}{\sqrt{(a + b)(c + d)(a + c)(b + d)}}\]
И \underline{коэффициент ассоциации $\acute{\text{Ю}}$ла} $Q$:
\[Q = \frac{ad - bc}{ad + bc}\]
Для коэффециента контингенции выполняется неравенство:
\[-1 \leq \Phi \leq 1\]
Если $\Phi = 1$:
\[\begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $a$ & $0$\\
    \hline
    0 & $0$ & $d$\\
    \hline
\end{tabular}\]
Если $\Phi = -1$:
\[\begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $0$ & $b$\\
    \hline
    0 & $c$ & $0$\\
    \hline
\end{tabular}\]
Для коэффициента Юла также выполняется неравенство:
\[-1 \leq Q \leq 1\]
Если $Q = 1$, тогда:
\[\begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $a$ & $0$\\
    \hline
    0 & $0$ & $d$\\
    \hline
\end{tabular},\ \text{или}\ \begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $a$ & $b$\\
    \hline
    0 & $0$ & $d$\\
    \hline
\end{tabular},\ \text{или}\ \begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $a$ & $0$\\
    \hline
    0 & $c$ & $d$\\
    \hline
\end{tabular}\]
Если $Q = -1$, тогда:
\[\begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $0$ & $b$\\
    \hline
    0 & $c$ & $0$\\
    \hline
\end{tabular},\ \text{или}\ \begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $a$ & $b$\\
    \hline
    0 & $c$ & $0$\\
    \hline
\end{tabular},\ \text{или}\ \begin{tabular}{|c|c|c|}
    \hline
    $A\setminus B$& 1 & 0\\
    \hline 
    1 & $0$ & $b$\\
    \hline
    0 & $c$ & $d$\\
    \hline
\end{tabular}\]
Также справедливо:
\[|\Phi| \leq |Q|\]
\subsection*{Пример}
Рассматривается связь между здоровьем зрения пациента до операции ($A$ --- число от 0 до 10) и наличием осложнений после операции ($B$ --- есть или нет). В результате испытаний была получена следующая таблица:
\[\begin{tabular}{|c|c|c|c|}
    \hline
    $A\setminus B$& нет & есть & \\
    \hline 
    $0-1$ & 129 & 14 & 143\\
    \hline
    $2-10$ & 807 & 4 & 811\\
    \hline
     & 936 & 18 & 954\\
     \hline
\end{tabular}\]
Проверяем $H_0: p_{i\, j} = p_{i\, \bullet} \cdot p_{\bullet\, j}$ против $H_1: p_{i\, j} \neq p_{i\, \bullet} \cdot p_{\bullet\, j}$.
\[\hat \chi^2 = \frac{954{(129\cdot 4 - 14\cdot 807)}^2}{143\cdot 811 \cdot 936 \cdot 18} \approx 51.8 \]
Должно выполняться
\[\hat \chi^2\Big|_{H_0} \sim \chi^2 (1)\]
Знаем квантиль $\chi^2_{0.95,\ 1} \approx 3.84 \Rightarrow$ принимается альтернативная гипотеза.\\
Посчитаем коэффициенты контингенции и Юла:
\[\begin{cases}
    \Phi = -0.244\\
    Q = -0.91
\end{cases}\]
Коэффициент Юла близок к $-1$, то есть среди людей с осложнениями после операции больше людей с изначально плохим зрением.
\subsection*{Определение}
Для таблиц произвольного размера определён \underline{коэффициент Пирсона}:
\[P = \sqrt{\frac{\hat \chi^2}{\hat \chi^2 + n}}\]
Если таблица порождена гауссовскими случайными величинами $(X_1,\ Y_1),\dots,\ (X_n,\ Y_n)$, то областью значений будет $\mathbb{R}^2$, для построения таблицы стоит разбить плоскость на прямоугольники, значением в ячейке таблицы будет количество точек, попавших в соответствующий прямоугольник. При большом количестве точек и хорошем разбиении выполняется:
\[\frac{\hat \chi^2}{\hat \chi^2 + n} \xrightarrow[n\to\infty]{} \hat \rho^2_{X Y}\]
Где $\hat \rho_{XY}^2$ --- выборочный коэффициент корреляции.\\
\subsection*{Определение}
Краммеру не понравилось, что коэффициент Пирсона никогда не равен 1 (а корреляция может быть 1), поэтому он придумал \underline{коэффициент Краммера}:
\[C = \sqrt{\frac{\hat \chi^2}{n \cdot \min \{ (k - 1),\ (m - 1) \}}}\]
Этот коэффициент уже может быть равен 1.

\begin{center}
    \bf Лекция 11 апреля
\end{center}
\section*{Работаем с порядковыми величинами}
\subsection*{Коэффициенты прогноза Гутмана $\lambda$-меры}
\subsubsection*{Пример}
Признак $A$ --- удовлетворённость уровнем жизни. $B$ --- материальное положение семьи.
\[\begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    $A\setminus B$ & плохое & ниже сред. & сред. & выше сред. & отл. & Всего\\
    \hline
    удовл. & 92 & 64 & 48 & 23 & 3 & 230 \\
    неудовл. & 22 & 46 & 136 & 148 & 73 & 425\\
    \hline
    Всего & 114 & 110 & 184 & 171 & 76 & 655\\
    \hline
\end{tabular}\]
Делаем прогноз относительно материального положения нового респондента. Пусть первый прогноз мы сделаем без знания признака $A$, а второй со знанием. При таких раскладах в первом случае наилучшим прогнозом будет самая многочисленная категория (сред.), а во втором --- при $A = \text{удовл.}$ наилучшим прогнозом будет самая многочисленная категория среди удовлетворённых (плохое), при $A = \text{неудовл.}$ --- выше среднего. Тогда вероятности ошибки в первом и втором прогнозе величины $B$:
\[\hat p_{1}^{(B)} = 1 - \frac{184}{655},\quad \hat p_{2}^{{B}} = 1 - \frac{92 + 148}{655} = 0.63\]
По этим вероятностям считается $\lambda$-мера (коэффициент Гутмана)
\[\lambda_{B} = \frac{\hat p_{1}^{(B)} - \hat p_2^{(B)}}{\hat p_{1}^{(B)}} \approx 0.119\]
$\lambda_B$ показывает, насколько увеличится вероятность угадывания, если будем учитывать знание значения другой категории.\\
Теперь попробуем спрогнозировать $A$ (удовлетворённость) без знания значения $B$ и с таким знанием:
\[\hat p_1^{(A)} = 1 - \frac{425}{655},\quad \hat p_2^{(A)} = 1 - \frac{92 + 64 + 136 + 148 + 73}{655} \Rightarrow \lambda_A = \frac{\hat p_1^{(A)} - \hat p_2^{(B)}}{\hat p_1^{(A)}} \approx 0.378\]
Иногда используется такой коэффициент:
\[\lambda = \frac{\lambda_A + \lambda_B}{2}\]
\subsubsection*{Пояснение}
Вероятности ошибок $\hat p_1^{(A)},\ \hat p_1^{(B)}$ равны вероятности события \{``Новый человек не принадлежит самой многочисленной категории''\}, поэтому они так считаются.\\
Вероятности ошибок $\hat p_2^{(A)},\ \hat p_2^{(B)}$ равны вероятности события \{``Новый человек не принадлежит самой многочисленной категории для своего уровня удовлетворённости/достатка''\}.
\subsubsection*{Обощение}
Теперь запишем формулу в общем виде:
\begin{equation*}
    \begin{aligned}
        &\hat \lambda_B = \frac{ 1 - \frac{\max\limits_j n_{\bullet\, j}}{n} - 1 + \sum\limits_{i} \frac{\max\limits_j n_{i\, j}}{n} }{1 - \frac{\max_{j} n_{\bullet\, j}}{n}} = \frac{\sum\limits_{i} \max\limits_{j} n_{i\, j} - \max\limits_j n_{\bullet\, j}}{ n - \max\limits_j n_{\bullet\, j} }\\
        &\hat p_1^{(A)} = 1 - \frac{\max\limits_i n_{i\, \bullet} }{n}\\
        &\hat p_2^{(A)} = 1 - \frac{\sum\limits_{j} \max\limits_{i} n_{i\, j}}{n}\\
        &\hat \lambda_A = \frac{\sum\limits_{j} \max\limits_{i} n_{i\, j} - \max\limits_i n_{i\, \bullet}}{ n - \max\limits_i n_{i\, \bullet} }\\
    \end{aligned}
\end{equation*}
\subsection*{Критерий Спирмена}
Применяется для выборок $(R_1,\ S_1),\dots,\ (R_n,\ S_n)$, где $R$ --- ранжированные значения первой переменной, $S$ --- ранжированные значения второй переменной.
\subsubsection*{Пример}
Исследовали способности детей к математике и музыки. Учителя соответствующих предметов ранжировали детей, получилась следующая таблица. Первая переменная --- ранги по знанию математики первой переменной, вторая --- ранг того же человека по музыке. 
\[
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
    \hline
    Математика & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
    \hline
    Музыка & 6  & 5 & 1 & 4 & 2 & 7 & 8 & 10 & 3 & 9\\   
    \hline
\end{tabular}
\]
\[S = \sum_{i = 1}^{n} {\left( R_i - S_i \right)}^2\]
Если $\forall i = \overline{1,\ n}\quad R_i = S_i \Rightarrow S = 0$\\
Если $\forall i = \overline{1,\ n}\quad S_i = n + 1 - R_i$ (то есть зависимость обратная), то $S = \frac{1}{3}\left( n^3 - n \right)$.
\[\rho_S = 1 - \frac{6\cdot \sum\limits_{i = 1}^{n} {\left( R_i - S_i \right)}^2}{n^3 - n}\]
\[-1 \leq \rho_S \leq 1\]
Выборочный коэффициент корреляции:
\[\hat \rho_{X\, Y} = \frac{ \sum\limits_{i = 1}^{n} \left( x_i - \overline{X} \right) \left( y_i - \overline{Y} \right) }{\sqrt{ \sum\limits_{i = 1}^{n} {\left( x_i - \overline{X} \right)}^2 \sum\limits_{i = 1}^{n} {\left( y_i - \overline{Y} \right)}^2 }}\]
\[\rho = \frac{\sum\limits_{i = 1}^{n} \left( R_i - \overline{R} \right) \left( S_i - \overline{S} \right)}{ \sqrt{ \sum\limits_{i = 1}^{n}{\left( R_i -\overline{R} \right)}^2 \sum\limits_{i = 1}^{n} {\left( S_i - \overline{S} \right)}^2 } }\]
\[\overline{R} = \overline{S} = \frac{n + 1}{2} \Rightarrow \sum\limits_{i = 1}^{n} {\left( i - \frac{n + 1}{2} \right)}^2\]
Если подставим числа, то получим ровно критерий Спирмена $\rho_S$.
\subsection*{Критерий Кендалла}
\subsubsection*{Понятия}
Имеется двумерная выборка $(X_1,\ Y_1),\dots,\ (X_n,\ Y_n)$, порождённая случайной величиной $z = (X,\ Y)$
\subsubsection*{Определение}
\underline{Параметром согласованности} случайных величин $X$ и $Y$ называется
\[\tau_{x\, y} = 1 - 2P\left( (x_2 - x_1)(y_2 - y_1) < 0 \right) \Rightarrow -1 \leq \tau_{x\, y} \leq 1\]
Здесь взяли индексы 1 и 2, потому что величины одинаково распределены и все $x$ независимы между собой (аналогично для $y$), то есть можем брать любые два различных элемента.\\
Если $y = \varphi(x)$ и $\varphi(x)$ строго возрастающая, то $\tau_{x\, y} = 1$\\
Если $y = \varphi(x)$ и $\varphi(x)$ строго убывающая, то $\tau_{x\, y} = -1$\\
Если $X,\ Y$ независимы, тогда:
\begin{equation*}
    \begin{aligned}
        \tau_{x\, y} &= 1 - 2P\left( (x_2 - x_1)(y_2 - y_1) < 0 \right) =\\
        & = 1 - 2 \left( P(x_2 - x_1 > 0) P(y_2 - y_1 < 0) + P(x_2 - x_1 < 0) P(y_2 - y_1 > 0) \right) = 1 - 2\left( \frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} \right) = \\
        & = 0      
    \end{aligned}
\end{equation*}
\subsubsection*{Пример}
$X \sim R(-1,\ 1)\quad Y = X^2$. Посчитаем параметр согласованности:
\[\tau_{X\, Y} = 1 - 2P\left( (X_2 - X_1)(X_2^2 - X_1^2) < 0 \right) = 1 - 2P\left( {\left( X_2 - X_1 \right)}^2 (X_2 + X_1) < 0\right) = 1 - 2P(X_2 + X_1 < 0) = 0\]
Вероятность равна нулю, потому что плотность распределения суммы двух симметричных отностельно 0 равномерных величин также будет симметрична относительно нуля. Вот красивый рисунок:
\begin{figure}[ht]
    \centering
    \caption*{График плотности распределения $R(-1,\ 1) + R(-1,\ 1)$}
    \begin{tikzpicture}
        \coordinate (0) at (-2, 0);
        \coordinate (1) at (2, 0);
        \coordinate (2) at (0, 1/2);
        \draw[-{latex}] (-4, 0) -- (4, 0) node[anchor = north] (x) {$x$};
        \draw[-{latex}] (0, 0-2) -- (0, 2) node[anchor = west] (y) {$y$};
        \filldraw[green, opacity=0.35] (0) -- (1) -- (2) -- cycle;
        \draw (0) node[anchor=north] {-2} -- (1) node[anchor=north] {2}  -- (2) node[anchor=south west]{0.5} -- cycle;
    \end{tikzpicture}
\end{figure}\\
То есть в этом случае величины зависимы, но параметр согласованности равен 0, потому что он отлавливает только монотонные зависимости, а квадратичная зависимость такой не является.\\
\subsubsection*{Гипотезы}
$H_0: \tau_{X\, Y} = 0$ против одной из $H_1: \tau_{X\, Y} < 0,\ H_2: \tau_{X\, Y} > 0,\ H_3: \tau_{X\, Y} \neq 0$. Для других альтернатив критерий несостоятелен.
\subsubsection*{Определение}
Пары $(X_i,\ Y_i)$ и $(X_j,\ Y_j)$ называются \underline{согласованными}, если
\[\sign (x_i - x_j)(y_i - y_j) = 1\]
И называются \underline{несогласованными}, если:
\[\sign (x_i - x_j)(y_i - y_j) = -1\]
Пусть в выборке есть $Q$ согласованных и $K$ несогласованных пар, рассмотрим следующую величину:
\[
S = Q - K
\]
Для этой неё выполняется неравенство:
\[
-\frac{n(n - 1)}{2} \leq S \leq \frac{n(n - 1)}{2}
\]
С помощью этой величины можно оценивать согласованность:
\[\hat \tau_{X\, Y} = \frac{S}{\max S} = \frac{2 (Q - K)}{n(n - 1)} = \left<Q = \frac{n(n - 1)}{2} - K \right> = \frac{2\left( \frac{n(n - 1)}{2} - K - K \right)}{n(n - 1)} = 1 - \frac{4K}{n(n - 1)}\]
Для $n \leq 40$ существует таблица квантилей $\hat \tau_{X\, Y}$ при справедливости $H_0$.\\
Если $n \to \infty$, выполняется:
\[\frac{3}{2}\sqrt{n} \hat \tau_{X\, Y}\Big|_{H_0} \sim N(0,\ 1)\]
\subsubsection*{Возвращаемся к примеру}
Решаем задачу про способности детей к математике и музыке. Проверяем гипотезу $H_0: \tau_{X\, Y} = 0$ против $H_1: \tau_{X\, Y} \neq 0$\\
Произведём расчёты:
\[ \sum_{i = 1}^{10} {\left( R_i - S_i \right)}^2 = 5^2 + 3^2 + 2^2 + 0 + 3^2 + 1 + 1 + 4 + 6^2 + 1 = 90 \]
Посчитаем статистику:
\[\rho_S = 1 - \frac{6\cdot 90}{10 (10^2 - 1)} \approx 0.45\]
Пусть $\alpha = 0.1$, нужен квантиль $\rho_{0,\ 0.95} = 0.564$, доверительной областью является $(-0.564,\ 0.564)$, значит по критерию Спирмана опровергнуть $H_0$ мы не можем, однако это обозначает либо недостаток испытаний, либо отсутствие монотонной зависимости.\\
Попробуем применить критерий Кендалла.\\
Посчитаем количество несогласованных пар:
\[K = 5 + 4 + 0 + 2 + 0 + 1 + 1 + 2 = 15\]
Тогда оценка параметра согласованности:
\[\tau_{X\, Y} = 1 - \frac{4\cdot 15}{10 \cdot 9} = \frac{1}{3}\]
Точный квантиль $\tau_{10,\ 0.95} = 0.422$, доверительной областью будет $(-0.422,\ 0.422)$, значит статистика попала в доверительный интервал. Критерий Кендалла также не позволил установить наличие монотонной зависимости, то есть данные необходимо отправить на дальнейшую проверку для установления зависимостей. 

\newpage
\begin{center}
    \bf Лекция 18 апреля
\end{center}
\section*{Исследование зависимости количественных показателей}
Для начала поработаем с двумерными случайными величинами, имеющими гауссовское распределение.\\
То есть рассматривается двумерная выборка $(X_1,\ Y_1),\dots,\ (X_n,\ Y_n)$, порождённая случайной величиной $z = (X,\ Y) \sim N(m_z,\ k_z)$.
\subsection*{Воспоминание}
Случайные величины $X,\ Y$ называются независимыми, если $\forall x,\ y \in \real^1:\quad F_z (x,\ y) = F_x(x)\cdot F_y(y)$.
\subsection*{Ещё одно воспоминание}
Если $z \sim N(m_z,\ k_z)$ и $\rho_{X\, Y} = 0 \Rightarrow$ случайные величины $X,\ Y$ независимы.\\
\textit{Замечание}: эта фишка работает только для гауссовских величин, в общем случае применять нельзя.
\subsubsection*{Доказательство}
Если $\rho_{X\, Y} = 0 \Rightarrow K_z = \begin{pmatrix}
    \sigma_x^2 & 0\\
    0 & \sigma_y^2
\end{pmatrix} \Rightarrow C = K_{z}^{-1} = \begin{pmatrix}
    \frac{1}{\sigma_x^2} & 0\\
    0 & \frac{1}{\sigma_y^2}
\end{pmatrix} \Rightarrow c_{1\, 2} = 0$.\\
Теперь распишем функцию плотности для величины $z$:
\[f_z(x,\ y) = \frac{\sqrt{\det C}}{ 2\pi} \exp \left\{ -\frac{1}{2} \left( {\left( x - m_x \right)}^2 c_{1\, 1} + {\left( y - m_y \right)}^2 c_{2\, 2} + 2c_{1\, 2} (x - m_x)(y - m_y) \right) \right\}\]
С нашей матрицей $C$ плотность можно переписать в виде:
\[f_z(x,\ y) = \frac{1}{2\pi \sigma_x \sigma_y} \exp \left\{ -\frac{{(x - m_x)}^2}{2\sigma_x^2} + \frac{ {\left( y - m_y \right)}^2 }{2\sigma_y^2} \right\} = \frac{1}{\sqrt{2\pi} \sigma_x} e^{-\frac{ {\left( x - m_x \right)}^2 }{2\sigma^2_x}} \frac{1}{\sqrt{2\pi} \sigma_y} e^{ -\frac{ {\left( y - m_y \right)}^2 }{2\sigma^2_y} } = f_x(x) f_y(y)\]
\subsection*{Возможные гипотезы}
Пусть $Z \sim N(m_z,\ K_z)$\\
Для гауссовских величин достаточно проверять гипотезу $H_0: \rho_{X\, Y} = 0$.\\
Возможные альтернативы:
\begin{enumerate}
    \item $H_1: \rho_{X\, Y} < 0$, то есть при возрастании одного показателя, второй убывает.
    \item $H_2: \rho_{X\, Y} > 0$, то есть при возрастании одного показателя, второй также возрастает.
    \item $H_3: \rho_{X\, Y} \neq 0$, то есть величины просто как-то зависимы.
\end{enumerate}
Для оценки корреляции можно использовать выборочный коэффициент корреляции:
\[
\hat \rho_{X\, Y} = \frac{ \sum_{i =  1}^{n} \left( x_i - \overline{X} \right) \left( y_i - \overline{Y} \right) }{ \sqrt{ \sum_{i = 1}^{n} {\left( x_i - \overline{X} \right)}^2 \sum_{i = 1}^{n} {\left( y_i - \overline{Y} \right)}^2 } }
\]
Умные люди придумали статистику и доказали для неё следующее:
\[
\left. \frac{\sqrt{n - 2} \hat \rho_{X\, Y}}{\sqrt{1 - \hat \rho^2_{X\, Y}}} \right|_{H_0} \sim t(n - 2)
\]
В этом случае основной гипотезой будет $H_0: \rho_{X\, Y} = 0$, против возможных альтернатив:
\begin{enumerate}
    \item Против гипотезы $H_1: \rho_{X\, Y} > 0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-2, 0) -- (1, 0);
    \draw[color=red] (1, 0) -- (2, 0);
    \draw (1, 0.15) -- (1, -0.15);
    \node at (1, -0.3) (a) {$t_{1 - \alpha}$};
\end{tikzpicture}\]
    \item Против гипотезы $H_2: \rho_{X\, Y} < 0$
\[\begin{tikzpicture}[scale = 2]
    \draw[color=green] (-1, 0) -- (2, 0);
    \draw[color=red] (-2, 0) -- (-1, 0);
    \draw (-1, -0.15) -- (-1, 0.15);
    \node at (-1, -0.3) (a) {$t_{\alpha}$};
\end{tikzpicture}\]
    \item Против гипотезы $H_3: \rho_{X\, Y} \neq 0$
\[
\begin{tikzpicture}[scale = 2]
    \draw[color=red] (-2, 0) -- (-1.5, 0);
    \draw[color=red] (1.5, 0) -- (2, 0);
    \draw[color=green] (-1.5, 0) -- (1.5, 0);
    \draw (1.5, -0.15) -- (1.5, 0.15);
    \draw (-1.5, -0.15) -- (-1.5, 0.15);
    \node at(-1.5, -0.3) (a) {$t_{\alpha/2}$};
    \node at(1.5, -0.3) (a) {$t_{1 - \alpha/2}$};
\end{tikzpicture}
\]
\end{enumerate}
При $n \to \infty$ справедливым будет:
\[
\sqrt{n} \hat \rho_{X\, Y} \Big|_{H_0} \sim N(0,\ 1)
\]
Рассмотрим свойства выборочного коэффициента корреляции:
\begin{equation*}
    \begin{aligned}
        & E\hat \rho_{X\, Y} = \rho_{X\, Y} - \rho_{X\, Y} \frac{ (1 - \rho_{X\, Y}^2) }{2n}\\
        & \dev \rho_{X\, Y} = \frac{ {\left( 1 - \rho_{X\, Y}^2 \right)}^2 }{n}
    \end{aligned}
\end{equation*}
В этих величинах используется неизвестная нам настоящая корреляция, поэтому её придётся заменять на оценку. При $n \to \infty$:
\[
\frac{\hat \rho_{X\, Y} - E\hat \rho_{X\, Y}}{\sqrt{\dev \hat\rho_{X\, Y}}} \sim N(0,\ 1)
\]
\subsection*{z-преобразования Фишера (наш слон?)}
Выглядят следующим образом (там гиперболический арктангенс):
\[
z = \operatorname{arcth} \rho = \frac{1}{2} \ln \frac{1 + \rho_{X\, Y}}{1 - \rho_{X\, Y}}
\]
В неё можно подставить выборочный коэффициент корреляции:
\[
\hat z = \frac{1}{2} \ln \frac{1 + \hat \rho_{X\, Y}}{1 - \hat \rho_{X\, Y}}
\]
Про параметры $\hat z$ известно:
\begin{equation*}
    \begin{aligned}
        & E \hat z = z + \frac{\rho_{X\, Y}}{2 (n - 1)}\\
        & \dev \hat z = \frac{1}{n - 3}
    \end{aligned}
\end{equation*}
При $n \to \infty$:
\[
\frac{\hat z - E\hat z}{\sqrt{\dev \hat z}} \sim N(0,\ 1)
\]
А ещё у величины $z$ прикольный гиперболический тангенс:
\[
\operatorname{th} z = \frac{e^z - e^{-z}}{e^z + e^{-z}} = \rho_{X\, Y}
\]
Сходимость к гауссовской величине здесь происходит быстрее, чем в предыдущем случае.
\subsection*{Адаптация критерия хи-квадрат}
\subsubsection*{Для дискретных величин}
Пусть случайная величина $X$ дискретная и принимает значения $a_1,\dots,\ a_m$,\\
Пусть случайная величина $Y$ дискретная и принимает значения $b_1,\dots,\ b_k$\\
По дискретным величинам мы умеем составлять таблицы:
\[
\begin{tabular}{|c|c|c|c|c|}
    \hline
    $X\setminus Y$ & $b_1$ & $\dots$ & $d_k$ & \\
    \hline
    $a_1$          & $n_{1\, 1}$ & $\dots$ & $n_{1\, k}$ & $n_{1\, \bullet}$\\
    \hline
    $\vdots$       & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\
    \hline
    $a_m$          & $n_{m\, 1}$ & $\dots$ & $n_{m\, k}$ & $n_{m\, \bullet}$\\
    \hline
                 & $n_{m\, \bullet}$ & $\dots$ & $n_{m\, \bullet}$ & $n$\\
    \hline
\end{tabular}
\]
\subsubsection*{Для непрерывных величин}
Если случайные величины $X,\ Y$ имеют непрерывное распределение, то нужно разбить их пространство значений (в нашем случае плоскость) на несколько непересекающихся интервалов, обладающих свойствами:
\begin{equation*}
    \begin{aligned}
        & \forall i,\ j:\quad i \neq j \Rightarrow \Delta x_i \cap \Delta x_j = \emptyset\\
        & \bigcup\limits_{i = 1}^{m} \Delta x_i = \real^1\\
        & \forall i,\ j:\quad i \neq j \Rightarrow \Delta y_i \cap \Delta y_j = \emptyset\\
        & \bigcup\limits_{i = 1}^{k} \Delta y_i = \real^1\\
    \end{aligned}
\end{equation*}
$n_{i\, j}$ --- количество наблюдений, попавших в прямоугольник $\Delta x_i \times \Delta y_i$.\\
По полученным данным строим таблицу:
\[
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $X\setminus Y$ & $\Delta y_1$ & $\dots$ & $\Delta y_k$ & \\
        \hline
        $\Delta x_1$          & $n_{1\, 1}$ & $\dots$ & $n_{1\, k}$ & $n_{1\, \bullet}$\\
        \hline
        $\vdots$       & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\
        \hline
        $\Delta x_m$          & $n_{m\, 1}$ & $\dots$ & $n_{m\, k}$ & $n_{m\, \bullet}$\\
        \hline
                     & $n_{m\, \bullet}$ & $\dots$ & $n_{m\, \bullet}$ & $n$\\
        \hline
    \end{tabular}
\]
Теперь к данным в таблицах можем применять уже известный нам критерий хи-квадрат для проверки гипотезы $H_0: \forall i = \overline{1,\ m}\ j = \overline{1,\ k}\quad P(X = a_{i},\ Y = b_{j}) = P(X = a_{i\, j}) P(y = b_j)$\\
Для проверки этой гипотезы можно использовать статистику:
\[
\hat \chi^2 = \sum_{i = 1}^{m} \sum_{j = 1}^{k} \frac{ { \left( n_{i\, j} - \frac{n_{i\, \bullet} n_{\bullet\, j}}{n} \right) }^2 }{\frac{n_{i\, \bullet} n_{\bullet\, j}}{n}}
\]
Для этой статистики выполняется:
\[
\hat \chi^2 \Big|_{H_0} \sim \chi^2\big( (k - 1)(m - 1) \big)
\]
Критическая область будет выглядеть следующим образом: $(\chi^2_{(k - 1)(m - 1),\ 1 - \alpha})$
\[\begin{tikzpicture}[scale=2]
    \draw[green] (-1, 0) -- (0, 0);
    \draw[green] (0, 0) -- (1.5, 0);
    \draw[red] (1.5, 0) -- (2, 0);
    \draw (0, -0.15) -- ++(0, 0.3);
    \node at (0, -0.3) (a) {$0$};
    \draw (1.5, -0.15) -- ++(0, 0.3);
    \node at (1.5, -0.3) (a) {$\chi^2_{1 - \alpha,\ (k - 1)(m - 1)}$};
\end{tikzpicture}\]
\textit{Замечание:} если в вашем разбиении получается так, что в каком-то из интервалов $\frac{n_{i\, \bullet} n_{\bullet\, j}}{n} < 3$, то его рекомендуется объединить с соседним.
\section*{Теперь работаем с больш$\acute{\textbf{и}}$м количеством переменных}
Под большим количеством понимается число больше 2.\\
\textit{Мотивация}: наличие корреляции между двумя величинами не означает наличие причинно-следственной связи между этими переменными (например, летом увеличивается количество мух и белых панамок, влияют ли панамки на мух?). Часто бывает так, что связь между исследуемыми нами переменными связана с изменением какой-то третьей (солнечной активности, в случае примера с мухами и панамками).
\subsection*{Определение}
\underline{Частные коэффициенты корреляции} для случайного вектора $z = (x_1,\dots,\ x_l)$ записываются в виде \underline{корреляционной матрицы}:
\[
R_z = \begin{pmatrix}
    \rho_{i\, j}
\end{pmatrix},\ \text{где } \forall i,\ j = \overline{1,\ l}\quad \rho_{i\, j} = \rho_{x_i\, x_j}
\]
\subsection*{Определение}
Частным коэффициентом корреляции случайных величин $X_1,\ X_2$ при фиксированных значениях $X_3,\dots,\ X_l$ называется:
\[
\rho_{1\, 2;\ 3,\dots,\ l} = \frac{-\mathbb{R}_{1\, 2}}{\sqrt{\mathbb{R}_{1\, 1}\mathbb{R}_{2\, 2} }}
\]
Здесь $\mathbb{R}_{i\, j}$ --- алгебраическое дополнение элемента $(i,\ j)$ матрицы $R_z$\\
\subsubsection*{Пример}
Пусть $z = (x_1,\ x_2,\ x_3),\ R_z = \begin{pmatrix}
    1 & \rho_{1\, 2} & \rho_{1,\ 3}\\
    \rho_{1\, 2} & 1 & \rho_{2,\ 3}\\
    \rho_{3\, 1} & \rho_{1\, 2} & 1\\
\end{pmatrix}$, тогда:
\[
\rho_{1\, 2;\ 3} = \frac{\rho_{2\, 1} - \rho_{2\, 3}\rho_{3\, 1}}{\sqrt{ (1 - \rho_{2\, 3}^2) (1 - \rho_{1\, 3}^2) }}
\]
Оценка матрицы $R_z$ --- матрица $\hat R_z = \begin{pmatrix}
    \hat \rho_{i\, j}
\end{pmatrix}$, состоящей из выборочных частных коэффициентов корреляции:
\[
\hat \rho_{1\, 2;\ 3} = \frac{\hat \rho_{2\, 1} - \hat\rho_{2\, 3} \hat\rho_{3\, 1}}{\sqrt{ (1 - \hat\rho_{2\, 3}^2) (1 - \hat\rho_{1\, 3}^2) }}
\]
\end{document}
